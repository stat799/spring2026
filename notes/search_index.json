[["index.html", "STAT 799 - Topics in Statistics: Applied linear mixed models in agriculture and life sciences Day 1 Welcome to STAT 799! 1.1 About this course: 1.2 Learning goals 1.3 On notation 1.4 Semester project 1.5 Roadmap of this course 1.6 Getting started with statistical modeling 1.7 Review 1.8 The most common statistical model 1.9 Uncertainty 1.10 Coming up tomorrow:", " STAT 799 - Topics in Statistics: Applied linear mixed models in agriculture and life sciences Josefina Lacasa Spring 2026 Day 1 Welcome to STAT 799! January 26th, 2026 1.1 About this course: About me About you: library(tidyverse) library(latex2exp) library(ggpubr) In rounds: What’s your major, what do you expect to learn? 1.1.1 Logistics Website Syllabus Statistical programming requirements Rough mindmap of the course (on whiteboard) Semester project Grades: Pass (100-69.99), Fail (&lt;69.99). Attendance policies Semester projects 1.2 Learning goals By the end of this course, you should be able to: Identify the data structure for a given dataset and write the statistical model that corresponds to said data structure using statistical notation. Distinguish the benefits and disadvantages of different modeling approaches. Write the Materials and Methods section in a paper (or graduate thesis) that describes the data generating process and the statistical model. 1.3 On notation scalars: \\(y\\), \\(\\sigma\\), \\(\\beta_0\\) vectors: \\(\\mathbf{y} \\equiv [y_1, y_2, ..., y_n]&#39;\\), \\(\\boldsymbol{\\beta} \\equiv [\\beta_1, \\beta_2, ..., \\beta_p]&#39;\\), \\(\\boldsymbol{u}\\) matrices: \\(\\mathbf{X}\\), \\(\\Sigma\\) probability distribution: \\(y \\sim N(0, \\sigma^2)\\), \\(\\mathbf{y} \\sim N(\\boldsymbol{0}, \\sigma^2\\mathbf{I})\\). 1.4 Semester project Manuscript See rubric on CANVAS. Publication-ready M&amp;M and results. Must include Abstract (250w), Introduction (~400-600w), M&amp;M (~300-700w), Results (~400w), Discussion (~300-500w), Conclusions (~100w). Last paragraph in the Introduction should clearly state the research gap and the research objectives. Reproducible Tutorial Publication-ready tutorial/R documentation. If you are not sure about your research question yet, talk to me after class today. 1.5 Roadmap of this course Table 1.1: Course Schedule: Statistical Modeling &amp; Mixed Models Date Topic Kahoot? 01/28 Statistical modeling. Continuous and categorical predictors. Review of mean, variance, and covariance. Types of uncertainty and sources of uncertainty. 01/29 Linear mixed models I. Model diagnostics and model selection. 01/30 Linear mixed models II. Model diagnostics and model selection. Yes Weekend NA NA 02/02 Analysis of variance. 02/03 Non-linear mixed models. 02/04 Troubleshooting in mixed models fitting using R software. Computational and analytical solutions. Yes 02/05 Generalized linear mixed models I. Beta, Binomial. 02/06 Statistical inference. Statistical power in designed experiments Weekend NA NA 02/09 Students are encouraged to attend Workshop ‘Non-linear models for the plant sciences’ 02/10 Students are encouraged to attend Workshop ‘Non-linear models for the plant sciences’ 02/11 Students are encouraged to attend Workshop ‘Non-linear models for the plant sciences’ 02/12 Scientific writing. Dos &amp; don’ts. 02/13 Wrap-up Yes 1.6 Getting started with statistical modeling Why do we need statistical models? Statistics as a summary of the data Excerpt from the short story “Funes the memorious” (J.L. Borges): I suspect, however, that he was not very capable of thought. To think is to forget differences, generalize, make abstractions. In the teeming world of Funes, there were only details, almost immediate in their presence. What is a statistical model? Deterministic component + Stochastic component 1.6.1 Writing a statistical model Deterministic component + Stochastic component Example: \\(y_i \\sim N(\\mu_i, \\sigma^2), \\\\ \\mu_i = \\beta_0 + \\beta_1 x_i\\) 1.6.2 What are linear models? Assume that \\(y_i \\sim N(\\mu_i, \\sigma^2)\\). …and there are 4 possible descriptions of the mean: \\(\\mu_i = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i}\\) \\(\\mu_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2\\) \\(\\mu_i = \\beta_0 \\cdot \\exp(x_i \\cdot \\beta_1)\\) \\(\\mu_i = \\beta_0 + x_i^{\\beta_1}\\) What is a linear model and what is not? 1.6.3 Vectorized notation Model equation form: \\(y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i\\) \\(\\varepsilon_i \\sim N(0, \\sigma^2),\\) Probability distribution form: Scalar \\(y_i \\sim N(\\mu_i, \\sigma^2),\\) \\(\\mu_i = \\beta_0 + \\beta_1 x_i\\) Vector \\(\\mathbf{y} \\sim N(\\boldsymbol{\\mu}, \\Sigma),\\) \\(\\boldsymbol{\\mu} = \\boldsymbol{1} \\beta_0 + \\mathbf{x} \\beta_1 = \\mathbf{X}\\boldsymbol{\\beta}\\) 1.6.4 Vectorized notation - cont. \\[\\begin{bmatrix}y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix} \\sim N \\left( \\begin{bmatrix}\\mu_1 \\\\ \\mu_2 \\\\ \\vdots \\\\ \\mu_n \\end{bmatrix}, \\begin{bmatrix} Cov(y_1, y_1) &amp; Cov(y_1, y_2) &amp; \\dots &amp; Cov(y_1, y_n) \\\\ Cov(y_2, y_1) &amp; Cov(y_2, y_2) &amp; \\dots &amp; Cov(y_2, y_n)\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ Cov(y_n, y_1) &amp; Cov(y_n, y_2) &amp; \\dots &amp; Cov(y_n, y_n) \\end{bmatrix} \\right)\\] 1.6.5 Continuous and categorical predictors \\[\\mu_i = \\beta_0 + x_{1i} \\beta_1 + x_{2i} \\beta_2+ ... + x_{ji} \\beta_j\\] Discuss the following type of data: biomass cultivar county year 1.6.6 Three-step approach to writing a statistical model 1. What probability distribution function describes the data? Think about: type of data, support of the data. 2. Define a link function for the linear predictor. 3. Define the linear predictor. Treatments Design 1.7 Review 1.7.1 Probability distributions knitr::include_graphics(&quot;figures/distributions.png&quot;) 1.7.2 Mean, Variance Mean: the balancing point of a probability distribution Variance: dispersion of \\(y\\) What do these distributions mean in practice? 1.7.3 Covariance Covariance between two random variables means how the two random variables behave relative to each other. Essentially, it quantifies the relationship between their joint variability. The variance of a random variable is the covariance of a random variable with itself. Consider two variables \\(y1\\) and \\(y2\\) each with a variance of 1 and a covariance of 0.6. \\[\\begin{bmatrix}y_1 \\\\ y_2 \\end{bmatrix} \\sim MVN \\left( \\begin{bmatrix} 10 \\\\ 8 \\end{bmatrix} , \\begin{bmatrix}1 &amp; 0.6 \\\\ 0.6 &amp; 1 \\end{bmatrix} \\right),\\] where the means of \\(y_1\\) and \\(y_2\\) are 10 and 8, respectively, and their covariance structure is represented in the variance-covariance matrix. Remember: \\[\\begin{bmatrix}y_1 \\\\ y_2 \\end{bmatrix} \\sim MVN \\left( \\begin{bmatrix} E(y_1) \\\\ E(y_2) \\end{bmatrix} , \\begin{bmatrix} Var(y_1) &amp; Cov(y_1, y_2) \\\\ Cov(y_2,y_2) &amp; Var(y_2) \\end{bmatrix} \\right).\\] 1.8 The most common statistical model \\[\\mathbf{y} \\sim N(\\boldsymbol\\mu, \\boldsymbol\\Sigma),\\] where: \\(\\mathbf{y} \\equiv [y_1, y_2, \\dots, y_n]&#39;\\) contains the response data, \\(\\boldsymbol{\\mu} \\equiv [\\mu_1, \\mu_2, \\dots, \\mu_n]&#39;\\) contains the expected values of said data, \\(\\boldsymbol\\Sigma\\) is the variance-covariance matrix. The most typical model typically has: \\(\\boldsymbol\\mu = \\mathbf{X}\\boldsymbol{\\beta}\\) and \\(\\boldsymbol\\Sigma = \\sigma^2\\mathbf{I}\\). In summary, the assumptions are: Normality Independence Constant variance 1.8.1 Properties of the general linear model \\(E(\\hat{\\boldsymbol{\\beta}}) = \\boldsymbol{\\beta}\\) \\(\\text{Var}(\\hat{\\boldsymbol{\\beta}}) = \\frac{\\sigma^2}{\\mathbf{X}^T\\mathbf{X}} = \\sigma^2 (\\mathbf{X}^T\\mathbf{X})^{-1}\\) Class discussion: What if the observations weren’t independent? What if the variance wasn’t constant? 1.9 Uncertainty Uncertainty will be one of the central topics in this course. Uncertainty is important because, as we summarize the information (e.g., to aviod Funes’ useless excess of information), we can better describe all the information with a combination of the mean and the variance. 1.9.1 Types of uncertainty Epistemological uncertainty Intrinsic uncertainty 1.9.2 Types of uncertainty in the general linear model Recall \\(\\mathbf{y} \\sim N(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\\), where \\(\\boldsymbol{\\mu} = \\mathbf{X}\\boldsymbol{\\beta}\\), and suppose \\(\\boldsymbol{\\Sigma} = \\sigma^2 \\mathbf{I}\\). We use the estimator of \\(\\boldsymbol{\\beta}\\), \\(\\hat{\\boldsymbol{\\beta}}\\). \\(Var(\\mathbf{y}) = \\sigma^2 = \\frac{SSE}{df_e}\\), \\(Var(\\hat{\\boldsymbol{\\beta}}) = \\frac{\\sigma^2}{\\mathbf{X}^T\\mathbf{X}}\\), and \\(Var(\\hat\\beta_1) = \\frac{\\sigma^2}{(n-1)s^2_{x_1}}\\). Discuss the connection between \\(n\\), \\({df_e}\\), \\(\\sigma^2\\) and \\(Var(\\hat{\\boldsymbol{\\beta}})\\). 1.9.3 Uncertainty - applied example Assume that the model \\(y_i \\sim N(\\mu_i, \\sigma^2), \\mu_i = \\beta_0 + \\beta_1 x_i+ \\beta_2 x_i^2\\) describes the data generating process. url &lt;- &quot;https://raw.githubusercontent.com/k-state-id3a/mixed-models-fall25/refs/heads/newpart3/data/nitrogen_yield.csv&quot; df_n_ss &lt;- read.csv(url) m1 &lt;- lm(Yield_SY ~ Total_N + I(Total_N^2), data = df_n_ss) DHARMa::simulateResiduals(m1, plot = T) ## Object of Class DHARMa with simulated residuals based on 250 simulations with refit = FALSE . See ?DHARMa::simulateResiduals for help. ## ## Scaled residual values: 0.1 0.3 0.004 0.588 0.792 0.556 0.168 0.344 0.256 0.452 0.388 0.388 0.832 0.94 0.692 0.936 0.132 0.312 0.424 0.664 ... m1 &lt;- lm(Yield_SY ~ Total_N + I(Total_N^2), data = df_n_ss) df_plot &lt;- data.frame(Total_N = 0:280) df_plot$yhat &lt;- predict(m1, newdata = df_plot) # estimation uncertainty confid_est &lt;- predict(m1, newdata = df_plot, se.fit = TRUE, interval = &quot;confidence&quot;) df_plot$est_low &lt;- (confid_est$fit)[,2] df_plot$est_up &lt;- (confid_est$fit)[,3] # estimation uncertainty confid_pred &lt;- predict(m1, newdata = df_plot, se.fit = TRUE, interval = &quot;predict&quot;) df_plot$pred_low &lt;- (confid_pred$fit)[,2] df_plot$pred_up &lt;- (confid_pred$fit)[,3] df_plot |&gt; ggplot(aes(Total_N, yhat))+ geom_ribbon(aes(ymin = pred_low, ymax = pred_up), alpha = .3, fill = &quot;#B388EB&quot;)+ geom_ribbon(aes(ymin = est_low, ymax = est_up), alpha = .6, fill = &quot;#B200EB&quot;)+ geom_line()+ labs(y = latex2exp::TeX(&quot;$\\\\hat{y}$&quot;), x = expression(Total~N~(lb~ac^{-1})))+ theme_pubclean() 1.10 Coming up tomorrow: Reach Ch1 in Stroup et al. (2024) We start with mixed models "],["mixed-models-i.html", "Day 2 Mixed models I 2.1 Recall the most common statistical model 2.2 Variations to that very common statistical model 2.3 Relaxing the assumption of independence 2.4 Fixed effects and random effects 2.5 Generalities on mixed models 2.6 Applied examples 2.7 Coming up tomorrow:", " Day 2 Mixed models I January 29th, 2026 2.1 Recall the most common statistical model \\[\\mathbf{y} \\sim N(\\boldsymbol\\mu, \\boldsymbol\\Sigma),\\] where: \\(\\mathbf{y} \\equiv [y_1, y_2, \\dots, y_n]&#39;\\) contains the response data, \\(\\boldsymbol{\\mu} \\equiv [\\mu_1, \\mu_2, \\dots, \\mu_n]&#39;\\) contains the expected values of said data, \\(\\boldsymbol\\Sigma\\) is the variance-covariance matrix. The most typical model typically has: \\(\\boldsymbol\\mu = \\mathbf{X}\\boldsymbol{\\beta}\\) and \\(\\boldsymbol\\Sigma = \\sigma^2\\mathbf{I}\\). We can write the default model in most software written above as: \\[\\mathbf{y} \\sim N(\\boldsymbol{\\mu}, \\Sigma),\\\\ \\begin{bmatrix}y_1 \\\\ y_2 \\\\ y_3 \\\\ y_4 \\\\ \\vdots \\\\ y_n \\end{bmatrix} \\sim N \\left( \\begin{bmatrix}\\mu_1 \\\\ \\mu_2 \\\\ \\mu_3 \\\\ \\mu_4 \\\\ \\vdots \\\\ \\mu_n \\end{bmatrix}, \\sigma^2 \\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 &amp; \\dots &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 &amp; \\dots &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; \\dots &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; \\dots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots\\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\dots &amp; 1 \\end{bmatrix} \\right),\\] which is the same as \\[\\begin{bmatrix}y_1 \\\\ y_2 \\\\ y_3 \\\\ y_4 \\\\ \\vdots \\\\ y_n \\end{bmatrix} \\sim N \\left( \\begin{bmatrix}\\mu_1 \\\\ \\mu_2 \\\\ \\mu_3 \\\\ \\mu_4 \\\\ \\vdots \\\\ \\mu_n \\end{bmatrix}, \\begin{bmatrix} \\sigma^2 &amp; 0 &amp; 0 &amp; 0 &amp; \\dots &amp; 0 \\\\ 0 &amp; \\sigma^2 &amp; 0 &amp; 0 &amp; \\dots &amp; 0 \\\\ 0 &amp; 0 &amp; \\sigma^2 &amp; 0 &amp; \\dots &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; \\sigma^2 &amp; \\dots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots\\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\dots &amp; \\sigma^2 \\end{bmatrix} \\right).\\] In summary, the assumptions are: Linearity (or whatever the deterministic equation is) Normality Independence Constant variance 2.1.1 Types of predictors Quantitative predictors Just like yesterday: \\[y_{i} \\sim N(\\mu_i, \\sigma^2), \\\\ \\mu_i = \\beta_0 + x_{1i} \\beta_1 + x_{2i} \\beta_2,\\] and say that \\(y_{i}\\) is the observed value for the \\(i\\)th observation, \\(\\beta_0\\) is the intercept, \\(x_{1i}\\) if the \\(i\\)th observation of \\(x_1\\), \\(\\beta_1\\) is the expected increase in \\(y\\) for each unit increase of \\(x_1\\), \\(x_{2i}\\) if the \\(i\\)th observation of \\(x_2\\), \\(\\beta_2\\) is the expected increase in \\(y\\) for each unit increase of \\(x_2\\). Qualitative predictors Instead of a quantitative predictor, we could have a qualitative (or categorical) predictor. Let the qualitative predictor have two possible levels, A and B. We could use the same model as before, \\[y_{i} \\sim N(\\mu_i, \\sigma^2), \\\\ \\mu_i = \\beta_0 + x_i \\beta_1,\\] and say that \\(y_{i}\\) is the observed value for the \\(i\\)th observation, \\(\\beta_0\\) is the expected value for A, \\(x_i = 0\\) if the \\(i\\)th observation belongs to A, and \\(x_i = 1\\) if the \\(i\\)th observation belongs to B. That way, \\(\\beta_1\\) is the difference between A and B. If the categorical predictor has more than two levels, \\[y_{i} \\sim N(\\mu_i, \\sigma^2), \\\\ \\mu_i = \\beta_0 + x_{1i} \\beta_1 + x_{2i} \\beta_2+ ... + x_{ji} \\beta_j,\\] \\(y_{i}\\) is still the observed value for the \\(i\\)th observation, \\(\\beta_0\\) is the expected value for A (sometimes in designed experiments this level is a control), \\[x_{1i} = \\begin{cases} 1, &amp; \\text{if } \\text{Treatment is B} \\\\ 0, &amp; \\text{if } \\text{else} \\end{cases}, \\\\ x_{2i} = \\begin{cases} 1, &amp; \\text{if } \\text{Treatment is C} \\\\ 0, &amp; \\text{if } \\text{else} \\end{cases}, \\\\ \\\\ \\dots, \\\\ \\\\ x_{ji} = \\begin{cases} 1, &amp; \\text{if } \\text{Treatment is J} \\\\ 1, &amp; \\text{if } \\text{else} \\end{cases}. \\] That way, all \\(\\beta\\)s are the differences between the treatment and the control. 2.2 Variations to that very common statistical model We can adapt the assumptions one by one: Linearity – change the deterministic equation Normality – assume a different distribution Independence – implement a hierarchical/multi-level/mixed model that models the data that were generated together. Constant variance – model the variance/assume a different distribution where the mean and the variance are not independent. 2.3 Relaxing the assumption of independence What if we can model the variance-covariance matrix with something else that’s not \\(\\sigma^2 \\mathbf{I}\\)? This is a visualization of the classical, default statistical model: This is a visualization of what’s coming with mixed models: Example 1: Example 2: 2.4 Fixed effects and random effects In what follows, we have a small elaboration of what it means to model that “structure in the data” (i.e., the groups of similarly generated points). 2.4.1 Going from fixed effects to fixed+random effects The data below were generated by an experiment that tested 18 sorghum genotypes in a randomized complete block design. Discuss the treatment structure Discuss the design structure dat_blocked &lt;- agridat::omer.sorghum |&gt; filter(env == &quot;E3&quot;) str(dat_blocked) ## &#39;data.frame&#39;: 72 obs. of 4 variables: ## $ env : Factor w/ 6 levels &quot;E1&quot;,&quot;E2&quot;,&quot;E3&quot;,..: 3 3 3 3 3 3 3 3 3 3 ... ## $ rep : Factor w/ 4 levels &quot;R1&quot;,&quot;R2&quot;,&quot;R3&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... ## $ gen : Factor w/ 18 levels &quot;G01&quot;,&quot;G02&quot;,&quot;G03&quot;,..: 1 2 3 4 5 6 7 8 9 10 ... ## $ yield: num 449 458 545 547 784 ... Now, remember that the blocks are supposed to indicate “groups of similar experimental units”. Said “groups of similar experimental units” means that the assumption of independence would be kind of a stretch. In reality, all observations from the same block (are supposed to) have something in common. It’s not reasonable to assume that the observations are independent, because observations from the same field have more in common than observations from different fields. They share more similar soil and, with that, a baseline fertility and yield. Basically, we expect the genotypes relative performance to be similar across fields, but the baseline (a.k.a., the intercept) to be field-specific. Then, we could say \\[y_{ij} = \\beta_{0j} + G_i + \\varepsilon_{ij}, \\\\ \\varepsilon_{ij} \\sim N(0, \\sigma^2),\\] where \\(\\beta_{0j}\\) is a block-specific intercept, and \\(G_i\\) is the genotype effect. Now, there are different ways to model that block-specific intercept. This is a big forking path in statistical modeling. All-fixed models estimate the effects of everything. Mixed-effects models indicate what is similar to what via random effects. Fixed effects We could define an all-fixed model, \\[y_{ij} = \\beta_{0j} + G_i + \\varepsilon_{ij}, \\\\ \\beta_{0j} = \\beta_0 + u_j \\\\ \\varepsilon_{ij} \\sim N(0, \\sigma^2),\\] where \\(u_j\\) is the effect of the \\(j\\)th block on the intercept (i.e., on the baseline). In this case, \\(u_j\\) is a fixed effect, which means it may be estimated via least squares estimation or maximum likelihood estimation. Under both least squares and maximum likelihood (assuming normal distribution), we may estimate the parameters by computing \\[\\hat{\\boldsymbol{\\beta}}_{ML} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y},\\] which yields the minimum variance unbiased estimate of \\(\\boldsymbol{\\beta}\\). We still have \\[\\boldsymbol{\\Sigma} = \\begin{bmatrix} \\sigma_{\\varepsilon}^2 &amp; 0 &amp; 0 &amp; 0 &amp; \\dots &amp; 0 \\\\ 0 &amp; \\sigma_{\\varepsilon}^2 &amp; 0 &amp; 0 &amp; \\dots &amp; 0 \\\\ 0 &amp; 0 &amp; \\sigma_{\\varepsilon}^2 &amp; 0 &amp; \\dots &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; \\sigma_{\\varepsilon}^2 &amp; \\dots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots\\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\dots &amp; \\sigma_{\\varepsilon}^2 \\end{bmatrix}\\]. m_fixed &lt;- lm(yield ~ gen + rep, data = dat_blocked) summary(m_fixed) ## ## Call: ## lm(formula = yield ~ gen + rep, data = dat_blocked) ## ## Residuals: ## Min 1Q Median 3Q Max ## -349.66 -81.58 -1.08 78.47 318.59 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 672.03 86.46 7.773 3.29e-10 *** ## genG02 -66.52 113.20 -0.588 0.559393 ## genG03 223.07 113.20 1.971 0.054201 . ## genG04 167.40 113.20 1.479 0.145336 ## genG05 61.05 113.20 0.539 0.591996 ## genG06 -267.20 113.20 -2.361 0.022111 * ## genG07 355.20 113.20 3.138 0.002826 ** ## genG08 159.53 113.20 1.409 0.164820 ## genG09 231.31 113.20 2.043 0.046198 * ## genG10 156.66 113.20 1.384 0.172393 ## genG11 146.29 113.20 1.292 0.202071 ## genG12 -42.87 113.20 -0.379 0.706453 ## genG13 243.18 113.20 2.148 0.036467 * ## genG14 1.06 113.20 0.009 0.992565 ## genG15 64.72 113.20 0.572 0.570007 ## genG16 22.81 113.20 0.201 0.841122 ## genG17 -296.66 113.20 -2.621 0.011534 * ## genG18 448.28 113.20 3.960 0.000233 *** ## repR2 -150.83 53.36 -2.827 0.006704 ** ## repR3 -85.66 53.36 -1.605 0.114592 ## repR4 -124.21 53.36 -2.328 0.023936 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 160.1 on 51 degrees of freedom ## Multiple R-squared: 0.6748, Adjusted R-squared: 0.5473 ## F-statistic: 5.291 on 20 and 51 DF, p-value: 7.64e-07 Random effects We could also assume that the effects of the \\(j\\)th block (i.e., \\(u_j\\)) arise from a random distribution. The most common assumption (and the default in most statistical software) is that \\[u_j \\sim N(0, \\sigma^2_b).\\] Now, we don’t estimate the effect, but the variance \\(\\sigma^2_b\\). Note that there are \\(J\\) levels of the random effects, meaning that a random effect is always categorical. Also, now \\[\\hat{\\boldsymbol{\\beta}}_{REML} = (\\mathbf{X}^T \\mathbf{V}^{-1} \\mathbf{X})^{-1}\\mathbf{X}^T \\mathbf{V}^{-1} \\mathbf{y},\\] where \\(\\mathbf{V} = Var(\\mathbf{y})\\) is the variance-covariance matrix of \\(\\mathbf{y}\\), including residual variance and random-effects variance. Note that this formula yields the same point estimate for \\(\\boldsymbol{\\beta}\\), but with a different confidence interval. Now, the variance-covariance matrix \\(\\boldsymbol{\\Sigma}\\) of the marginal distribution has changed. library(lme4) m_mixed &lt;- lmer(yield ~ gen + (1|rep), data = dat_blocked) summary(m_mixed) ## Linear mixed model fit by REML [&#39;lmerMod&#39;] ## Formula: yield ~ gen + (1 | rep) ## Data: dat_blocked ## ## REML criterion at convergence: 729.7 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -1.99895 -0.58043 -0.01854 0.52108 1.92022 ## ## Random effects: ## Groups Name Variance Std.Dev. ## rep (Intercept) 2906 53.91 ## Residual 25627 160.09 ## Number of obs: 72, groups: rep, 4 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) 581.86 84.46 6.889 ## genG02 -66.52 113.20 -0.588 ## genG03 223.07 113.20 1.971 ## genG04 167.40 113.20 1.479 ## genG05 61.05 113.20 0.539 ## genG06 -267.20 113.20 -2.361 ## genG07 355.20 113.20 3.138 ## genG08 159.53 113.20 1.409 ## genG09 231.31 113.20 2.043 ## genG10 156.66 113.20 1.384 ## genG11 146.29 113.20 1.292 ## genG12 -42.87 113.20 -0.379 ## genG13 243.18 113.20 2.148 ## genG14 1.06 113.20 0.009 ## genG15 64.72 113.20 0.572 ## genG16 22.81 113.20 0.201 ## genG17 -296.66 113.20 -2.621 ## genG18 448.28 113.20 3.960 ## ## Correlation matrix not shown by default, as p = 18 &gt; 12. ## Use print(x, correlation=TRUE) or ## vcov(x) if you need it 2.5 Generalities on mixed models Mixed models combine fixed effects and random effects. Generally speaking, we can write out a mixed-effects model using the model equation form, as \\[\\mathbf{y} = \\mathbf{X} \\boldsymbol{\\beta} + \\mathbf{Z}\\mathbf{u} + \\boldsymbol{\\varepsilon}, \\\\ \\begin{bmatrix}\\mathbf{u} \\\\ \\boldsymbol{\\varepsilon} \\end{bmatrix} \\sim \\left( \\begin{bmatrix}\\boldsymbol{0} \\\\ \\boldsymbol{0} \\end{bmatrix}, \\begin{bmatrix}\\mathbf{G} &amp; \\boldsymbol{0} \\\\ \\boldsymbol{0} &amp; \\mathbf{R} \\end{bmatrix} \\right),\\] where \\(\\mathbf{y}\\) is the observed response, \\(\\mathbf{X}\\) is the matrix with the explanatory variables, \\(\\mathbf{Z}\\) is the design matrix, \\(\\boldsymbol{\\beta}\\) is the vector containing the fixed-effects, \\(\\mathbf{u}\\) is the vector containing the random effects, \\(\\boldsymbol{\\varepsilon}\\) is the vector containing the residuals, \\(\\mathbf{G}\\) is the variance-covariance matrix of the random effects, and \\(\\mathbf{R}\\) is the variance-covariance matrix of the residuals. Note that \\(\\mathbf{X} \\boldsymbol{\\beta}\\) is the fixed effects part of the model, and \\(\\mathbf{Z}\\mathbf{u}\\) is the random effects part of the model. Using the probability distribution form, we can then say that \\(E(\\mathbf{y}) = \\mathbf{X}\\boldsymbol{\\beta}\\) and \\(Var(\\mathbf{y}) = \\mathbf{V} = \\mathbf{Z}\\mathbf{G}\\mathbf{Z}&#39; + \\mathbf{R}\\). Usually, we assume \\[\\mathbf{G} = \\sigma^2_u \\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; \\dots 0 \\\\ 0 &amp; 1 &amp; 0 &amp; \\dots &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; \\dots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; 0 &amp; \\dots &amp; 1 \\end{bmatrix}\\] and \\[\\mathbf{R} = \\sigma^2 \\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; \\dots &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; \\dots &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; \\dots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; 0 &amp; \\dots &amp; 1 \\end{bmatrix} \\]. Then, \\[\\mathbf{y} \\sim N(\\boldsymbol{\\mu}, \\Sigma), \\\\ \\Sigma = \\begin{bmatrix} \\sigma^2 + \\sigma^2_u &amp; \\sigma^2_u &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp;\\dots &amp; 0\\\\ \\sigma^2_u &amp; \\sigma^2 + \\sigma^2_u &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\dots &amp; 0 \\\\ 0 &amp; 0 &amp; \\sigma^2 + \\sigma^2_u &amp; \\sigma^2_u &amp; 0 &amp; 0 &amp; \\dots &amp; 0 \\\\ 0 &amp; 0 &amp; \\sigma^2_u &amp; \\sigma^2 + \\sigma^2_u &amp; 0 &amp; 0 &amp; \\dots &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\sigma^2 + \\sigma^2_u &amp; \\sigma^2_u &amp; \\dots &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\sigma^2_u &amp; \\sigma^2 + \\sigma^2_u &amp; \\dots &amp; \\vdots \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\dots &amp; \\sigma^2 + \\sigma^2_u \\end{bmatrix}.\\] Take your time to digest the variance-covariance matrix above. What type of data do you think generated it? 2.5.1 Random effects By definition, random effects are regression coefficients that arise from a random distribution. Typically, a random effect \\(u \\sim N(0, \\sigma^2_u)\\). Note that this model for the parameter may result in shrinkage. We estimate the variance \\(\\sigma^2_u\\). Calculating degrees of freedom can get much more complex than in all-fixed effects models (e.g., with unbalanced data, spatio-temporally correlated data, or non-normal data). In the context of designed experiments, random effects are assumed to be independent to each other and independent to the residual. 2.5.2 Estimation of parameters “Estimation” is a term held mostly exclusive to fixed effects and variance components. Restricted maximum likelihood estimation (REML) is the default in most mixed effects models because, for small data (aka most experimental data), maximum likelihood (ML) provides variance estimates that are downward biased. - In REML, the likelihood is maximized after accounting for the model’s fixed effects. In ML, \\(\\ell_{ML}(\\boldsymbol{\\sigma; \\boldsymbol{\\beta}, \\mathbf{y}}) = - (\\frac{n}{2}) \\log(2\\pi)-(\\frac{1}{2}) \\log ( \\vert \\mathbf{V}(\\boldsymbol\\sigma) \\vert ) - (\\frac{1}{2}) (\\mathbf{y}-\\mathbf{X}\\boldsymbol{\\beta})^T[\\mathbf{V}(\\boldsymbol\\sigma)]^{-1}(\\mathbf{y}-\\mathbf{X}\\boldsymbol{\\beta})\\) In REML, \\(\\ell_{REML}(\\boldsymbol{\\sigma};\\mathbf{y}) = - (\\frac{n-p}{2}) \\log (2\\pi) - (\\frac{1}{2}) \\log ( \\vert \\mathbf{V}(\\boldsymbol\\sigma) \\vert ) - (\\frac{1}{2})log \\left( \\vert \\mathbf{X}^T[\\mathbf{V}(\\boldsymbol\\sigma)]^{-1}\\mathbf{X} \\vert \\right) - (\\frac{1}{2})\\mathbf{r}[\\mathbf{V}(\\boldsymbol\\sigma)]^{-1}\\mathbf{r}\\), where \\(p = rank(\\mathbf{X})\\), \\(\\mathbf{r} = \\mathbf{y}-\\mathbf{X}\\hat{\\boldsymbol{\\beta}}_{ML}\\). Start with initial values for \\(\\boldsymbol{\\sigma}\\), \\(\\tilde{\\boldsymbol{\\sigma}}\\). Compute \\(\\mathbf{G}(\\tilde{\\boldsymbol{\\sigma}})\\) and \\(\\mathbf{R}(\\tilde{\\boldsymbol{\\sigma}})\\). Obtain \\(\\boldsymbol{\\beta}\\) and \\(\\mathbf{b}\\). Update \\(\\tilde{\\boldsymbol{\\sigma}}\\). Repeat until convergence. 2.5.3 Fixed effects versus random effects What is behind a random effect: $ N ( , (^T ^{-1} )^{-1} ) $ \\(u_j \\sim N(0, \\sigma^2_u)\\) What process is being studied? How were the levels selected? (randomly, carefully selected) How many levels does the factor have, vs. how many did we observe? BLUEs versus BLUPs. Read more in in Gelman (2005, page 20), “Analysis of variance—why it is more important than ever” [link], and Gelman and Hill (2006), page 245. 2.6 Applied examples Three sets of data describe the relationship between treatment and crop yield. However, the structures in the data (i.e., data architecture) are different. 2.6.1 Example A – independence holds dat_independent &lt;- read.csv(&quot;../data/cochrancox_kfert.csv&quot;) m_independent &lt;- lm(yield ~ factor(K2O_lbac), data = dat_independent) 2.6.2 Example B – simple groups of similar observations library(lme4) dat_blocked &lt;- agridat::omer.sorghum |&gt; filter(env == &quot;E3&quot;) m_blocked &lt;- lmer(yield ~ gen + (1|rep), data = dat_blocked) 2.6.3 Example C – different groups of similar observations dat_multilevel &lt;- agridat::durban.splitplot m_multilevel &lt;- lmer(yield ~ gen*fung + (1|block/fung), data = dat_multilevel) 2.7 Coming up tomorrow: Review on mixed models Model diagnostics and model comparison Kahoot "],["mixed-models-ii.html", "Day 3 Mixed models II 3.1 Generalities of linear mixed models 3.2 Inference from linear mixed models 3.3 Coming up Monday:", " Day 3 Mixed models II January 30th, 2026 3.1 Generalities of linear mixed models Mixed-effects models combine fixed effects and random effects. Typically, we can define a Gaussian mixed-effects model as \\[\\mathbf{y} = \\mathbf{X} \\boldsymbol{\\beta} + \\mathbf{Z}\\mathbf{u} + \\boldsymbol{\\varepsilon}, \\\\ \\begin{bmatrix}\\mathbf{u} \\\\ \\boldsymbol{\\varepsilon} \\end{bmatrix} \\sim \\left( \\begin{bmatrix}\\boldsymbol{0} \\\\ \\boldsymbol{0} \\end{bmatrix}, \\begin{bmatrix}\\mathbf{G} &amp; \\boldsymbol{0} \\\\ \\boldsymbol{0} &amp; \\mathbf{R} \\end{bmatrix} \\right),\\] where \\(\\mathbf{y}\\) is the observed response, \\(\\mathbf{X}\\) is the matrix with the explanatory variables, \\(\\mathbf{Z}\\) is the design matrix, \\(\\boldsymbol{\\beta}\\) is the vector containing the fixed-effects parameters, \\(\\mathbf{u}\\) is the vector containing the random effects parameters, \\(\\boldsymbol{\\varepsilon}\\) is the vector containing the residuals, \\(\\mathbf{G}\\) is the variance-covariance matrix of the random effects, and \\(\\mathbf{R}\\) is the variance-covariance matrix of the residuals. Typically, \\(\\mathbf{G} = \\sigma^2_u \\mathbf{I}\\) and \\(\\mathbf{R} = \\sigma^2 \\mathbf{I}\\). If we do the math, we get that \\[E(\\mathbf{y}) = \\mathbf{X}\\boldsymbol{\\beta},\\] \\[Var(\\mathbf{y}) = \\mathbf{Z}\\mathbf{G}\\mathbf{Z}&#39; + \\mathbf{R}.\\] Fixed effects versus random effects Fixed vs Random Effects Table table.unique-table { width: 100%; border-collapse: collapse; margin: 20px 0; } table.unique-table th, table.unique-table td { border: 1px solid #ddd; padding: 8px; text-align: left; } table.unique-table th { background-color: #f4f4f4; font-weight: bold; } table.unique-table tr:nth-child(even) { background-color: #f9f9f9; } Fixed effects Random effects Where Expected value Variance-covariance matrix Inference Constant for all groups in the population of study Differ from group to group Usually used to model Carefully selected treatments or genotypes The study design (aka structure in the data, or what is similar to what) Assumptions \\[\\hat{\\boldsymbol{\\beta}} \\sim N \\left( \\boldsymbol{\\beta}, (\\mathbf{X}^T \\mathbf{V}^{-1} \\mathbf{X})^{-1} \\right) \\] \\[u_j \\sim N(0, \\sigma^2_u)\\] Method of estimation Maximum likelihood, least squares Restricted maximum likelihood (shrinkage) 3.2 Inference from linear mixed models 3.2.1 Balanced designs – blocks as fixed or as random? Fixed vs Random Blocks table.unique-table { width: 100%; border-collapse: collapse; margin: 20px 0; } table.unique-table th, table.unique-table td { border: 1px solid #ddd; padding: 8px; text-align: left; } table.unique-table th { background-color: #f4f4f4; font-weight: bold; } table.unique-table tr:nth-child(even) { background-color: #f9f9f9; } Fixed effects Random effects Standard error of the mean \\(\\sqrt{\\frac{\\sigma^2_\\varepsilon}{b}}\\) \\(\\sqrt{\\frac{\\sigma^2_\\varepsilon + \\sigma^2_b}{b}}\\) Stanard error of the difference of means \\(\\sqrt{\\frac{2\\sigma^2_\\varepsilon}{b}}\\) \\(\\sqrt{\\frac{2\\sigma^2_\\varepsilon}{b}}\\) See Dixon (2016). library(lme4) library(tidyverse) library(emmeans) library(latex2exp) df &lt;- read.csv(&quot;../data/cochrancox_kfert.csv&quot;) df$rep &lt;- as.factor(df$rep) df$K2O_lbac &lt;- as.factor(df$K2O_lbac) m_fixed &lt;- lm(yield ~ K2O_lbac + rep, data = df) m_random &lt;- lmer(yield ~ K2O_lbac + (1|rep), data = df) (mg_means_fixed &lt;- emmeans(m_fixed, ~K2O_lbac, contr = list(c(1, 0, 0, 0, -1)))) ## $emmeans ## K2O_lbac emmean SE df lower.CL upper.CL ## 36 7.92 0.121 8 7.64 8.19 ## 54 8.12 0.121 8 7.84 8.40 ## 72 7.81 0.121 8 7.53 8.09 ## 108 7.58 0.121 8 7.30 7.86 ## 144 7.52 0.121 8 7.24 7.79 ## ## Results are averaged over the levels of: rep ## Confidence level used: 0.95 ## ## $contrasts ## contrast estimate SE df t.ratio p.value ## c(1, 0, 0, 0, -1) 0.4 0.171 8 2.344 0.0471 ## ## Results are averaged over the levels of: rep (mg_means_random &lt;- emmeans(m_random, ~K2O_lbac, contr = list(c(1, 0, 0, 0, -1)))) ## $emmeans ## K2O_lbac emmean SE df lower.CL upper.CL ## 36 7.92 0.162 5.57 7.51 8.32 ## 54 8.12 0.162 5.57 7.72 8.52 ## 72 7.81 0.162 5.57 7.41 8.21 ## 108 7.58 0.162 5.57 7.18 7.98 ## 144 7.52 0.162 5.57 7.11 7.92 ## ## Degrees-of-freedom method: kenward-roger ## Confidence level used: 0.95 ## ## $contrasts ## contrast estimate SE df t.ratio p.value ## c(1, 0, 0, 0, -1) 0.4 0.171 8 2.344 0.0471 ## ## Degrees-of-freedom method: kenward-roger as.data.frame(mg_means_fixed$emmeans) %&gt;% mutate(blocks = &quot;fixed&quot;) %&gt;% bind_rows(as.data.frame(mg_means_random$emmeans) %&gt;% mutate(blocks = &quot;random&quot;)) %&gt;% ggplot(aes(K2O_lbac, emmean))+ geom_errorbar(aes(ymin = emmean-SE, ymax = emmean+SE, color = blocks), position = position_dodge(width = .2), width = 0)+ geom_text(aes(x = 4.6, y = 8), label = &quot;s.e.(difference\\nbetween means)&quot;, color = &quot;grey30&quot;, size = 3.5)+ geom_point(aes(x = 5.2, y = 8), color = &quot;grey50&quot;)+ geom_point(aes(x = 5.4, y = 8), color = &quot;grey50&quot;)+ geom_errorbar(aes(x = 5.4, y = 8, ymin = 8 - SE, ymax = 8 + SE), width = 0, color = &quot;grey50&quot;, data = as.data.frame(mg_means_random$contrasts))+ geom_errorbar(aes(x = 5.2, y = 8, ymin = 8 - SE, ymax = 8 + SE), width = 0, color = &quot;grey50&quot;, data = as.data.frame(mg_means_fixed$contrasts))+ scale_color_manual(values = c(&quot;grey30&quot;, &quot;tomato&quot;))+ geom_point(aes(color = blocks), position = position_dodge(width = .2))+ theme_classic()+ labs(title = &quot;Spoiler: difference in results for RCBD data\\nmodeled with fixed vs. random blocks&quot;, color = &quot;Blocks modeled as&quot;, y = expression(Yield~(tn~ac^{-1})), x = expression(K[2]~O~(lb~ac^{-1})))+ theme(legend.position = &quot;bottom&quot;, plot.title = element_text(hjust = .5)) 3.2.2 Unbalanced designs Unbalanced designs sometimes occur due to logistical (practical) convenience. In unbalanced designs, the simple comparison of group averages, \\(\\bar{y}_1 - \\bar{y}_0\\), is not, in general, a good estimate of the average treatment effect. See Chapter 10 in Gelman and Hill. Multilevel (mixed) models are useful to recover inter-group information. Intra-block information: differences between treatments inside the same block. Inter-block information: differences between the totals of blocks containing different treatments. If you’re new to this: Mixed models are better at recovering inter-block information (i.e., comparing across blocks, even though maybe blocks don’t share the same treatments) than all-fixed effects models. Recovering inter-block information means that they have more information about, for e.g., mean comparisons. Then, mean comparisons are more precise (i.e., more narrow CI). If you’re not new to this: Yates (1940): Fixed-only models only recover intra-block information. To recover inter-block information, we can weigh the means depending on what block they fell in “(…)estimates of the varietal differences will be given by the differences of the weighted means.” Patterson and Thompson (1971) discuss the recovery of inter-block information when block sizes are unequal. Henderson’s mixed models are already around to estimate the weights for the means. Where are those weights you ask? \\(\\mathbf{V}\\) in the estimator \\(\\hat{\\boldsymbol{\\beta}}_{REML} = (\\mathbf{X}^T \\mathbf{V}^{-1} \\mathbf{X})^{-1}\\mathbf{X}^T \\mathbf{V}^{-1} \\mathbf{y}.\\) Weights are inversely proportional to \\(\\sigma^2_b\\). Let’s look at an example The data below were generated by a balanced incomplete block design, where genotypes are the treatment factor (one-way trt structure with 13 levels) and the locations are the blocking factor. As discussed, there are two obvious candidate models: All-fixed model \\[y_{ij} = \\mu + G_i + L_j + \\varepsilon_{ij}, \\\\ \\varepsilon_{ij} \\sim N(0, \\sigma^2).\\] Mixed model \\[y_{ij} = \\mu + G_i + L_j + \\varepsilon_{ij}, \\\\ L_{j} \\sim N(0, \\sigma_L^2), \\\\ \\varepsilon_{ij} \\sim N(0, \\sigma^2).\\] Let’s fit those models to the data and see what we get: library(lme4) library(emmeans) # data generated by an BIBD dat_bibd &lt;- agridat::cochran.bib # all-fixed option m_fixed_intra &lt;- lm(yield ~ gen + loc , data = dat_bibd) # mixed option m_mixed_inter &lt;- lmer(yield ~ gen + (1|loc) , data = dat_bibd) Let’s look at a summary of both. Note that the genotype effects don’t match like they would for an RCBD. summary(m_fixed_intra) ## ## Call: ## lm(formula = yield ~ gen + loc, data = dat_bibd) ## ## Residuals: ## Min 1Q Median 3Q Max ## -7.2173 -2.0077 0.0404 1.7942 7.6673 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 29.9865 3.5567 8.431 4.83e-09 *** ## genG02 -4.7308 3.5024 -1.351 0.1880 ## genG03 -2.7846 3.5024 -0.795 0.4335 ## genG04 -4.9000 3.5024 -1.399 0.1732 ## genG05 -3.0462 3.5024 -0.870 0.3921 ## genG06 -5.9000 3.5024 -1.685 0.1036 ## genG07 -3.2769 3.5024 -0.936 0.3578 ## genG08 0.7154 3.5024 0.204 0.8397 ## genG09 -3.9846 3.5024 -1.138 0.2653 ## genG10 -4.9769 3.5024 -1.421 0.1668 ## genG11 -8.4769 3.5024 -2.420 0.0225 * ## genG12 -2.9154 3.5024 -0.832 0.4125 ## genG13 2.3769 3.5024 0.679 0.5031 ## locB02 -2.8154 3.5024 -0.804 0.4285 ## locB03 -3.0385 3.5024 -0.868 0.3933 ## locB04 0.7231 3.5024 0.206 0.8380 ## locB05 2.1692 3.5024 0.619 0.5409 ## locB06 5.4692 3.5024 1.562 0.1300 ## locB07 3.0000 3.5024 0.857 0.3992 ## locB08 5.9462 3.5024 1.698 0.1011 ## locB09 8.0615 3.5024 2.302 0.0293 * ## locB10 3.2231 3.5024 0.920 0.3656 ## locB11 5.9769 3.5024 1.707 0.0994 . ## locB12 4.3154 3.5024 1.232 0.2285 ## locB13 6.1692 3.5024 1.761 0.0895 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.465 on 27 degrees of freedom ## Multiple R-squared: 0.6541, Adjusted R-squared: 0.3467 ## F-statistic: 2.128 on 24 and 27 DF, p-value: 0.02976 summary(m_mixed_inter) ## Linear mixed model fit by REML [&#39;lmerMod&#39;] ## Formula: yield ~ gen + (1 | loc) ## Data: dat_bibd ## ## REML criterion at convergence: 253.6 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -1.9920 -0.5298 0.1450 0.5707 1.5626 ## ## Random effects: ## Groups Name Variance Std.Dev. ## loc (Intercept) 6.053 2.460 ## Residual 19.934 4.465 ## Number of obs: 52, groups: loc, 13 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) 34.171 2.445 13.978 ## genG02 -5.131 3.333 -1.539 ## genG03 -4.063 3.333 -1.219 ## genG04 -6.095 3.333 -1.829 ## genG05 -3.828 3.333 -1.149 ## genG06 -6.579 3.333 -1.974 ## genG07 -3.414 3.333 -1.024 ## genG08 -1.419 3.333 -0.426 ## genG09 -5.616 3.333 -1.685 ## genG10 -6.071 3.333 -1.821 ## genG11 -10.703 3.333 -3.211 ## genG12 -5.185 3.333 -1.556 ## genG13 1.004 3.333 0.301 Look at the variance. sigma(m_fixed_intra) ## [1] 4.464749 sigma(m_mixed_inter) ## [1] 4.464749 Look at some treatment means. Why are all s.e. the same? Why are s.e.(fixed) &lt; s.e.(random)? head(as.data.frame(emmeans(m_fixed_intra, ~gen))) ## gen emmean SE df lower.CL upper.CL ## G01 33.00192 2.458672 27 27.95715 38.04670 ## G02 28.27115 2.458672 27 23.22638 33.31593 ## G03 30.21731 2.458672 27 25.17253 35.26209 ## G04 28.10192 2.458672 27 23.05714 33.14670 ## G05 29.95577 2.458672 27 24.91099 35.00055 ## G06 27.10192 2.458672 27 22.05714 32.14670 ## ## Results are averaged over the levels of: loc ## Confidence level used: 0.95 head(as.data.frame(emmeans(m_mixed_inter, ~gen))) ## gen emmean SE df lower.CL upper.CL ## G01 34.17116 2.496721 38.32 29.11820 39.22412 ## G02 29.04064 2.496721 38.32 23.98769 34.09360 ## G03 30.10793 2.496721 38.32 25.05497 35.16089 ## G04 28.07579 2.496721 38.32 23.02283 33.12875 ## G05 30.34293 2.496721 38.32 25.28998 35.39589 ## G06 27.59169 2.496721 38.32 22.53873 32.64464 ## ## Degrees-of-freedom method: kenward-roger ## Confidence level used: 0.95 Look at some treatment differences. Why are s.e.(fixed) &gt; s.e.(random)? emmeans(m_fixed_intra, ~gen, contr = list(c(1, -1, rep(0, 11))))$contrasts ## contrast estimate SE df t.ratio p.value ## c(1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0) 4.73 3.5 27 1.351 0.1880 ## ## Results are averaged over the levels of: loc emmeans(m_mixed_inter, ~gen, contr = list(c(1, -1, rep(0, 11))))$contrasts ## contrast estimate SE df t.ratio p.value ## c(1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0) 5.13 3.42 32.6 1.502 0.1427 ## ## Degrees-of-freedom method: kenward-roger 3.3 Coming up Monday: A different look at ANOVA Statistical inference "],["mixed-models-iii-model-checking-anova-statistical-inference.html", "Day 4 Mixed models III – model checking, ANOVA, statistical inference 4.1 Review 4.2 Model checking and comparison 4.3 Model checking 4.4 Simulation-based model-checking 4.5 Some useful metrics to compare models 4.6 Analysis of variance – ANOVA 4.7 Critiques of ANOVA over the years 4.8 A different take on ANOVA using multilevel modeling 4.9 Coming up tomorrow:", " Day 4 Mixed models III – model checking, ANOVA, statistical inference February 2nd, 2026 4.1 Review Mixed-effects models combine fixed effects and random effects. Typically, we can define a Gaussian mixed-effects model as \\[\\mathbf{y} = \\mathbf{X} \\boldsymbol{\\beta} + \\mathbf{Z}\\mathbf{u} + \\boldsymbol{\\varepsilon}, \\\\ \\begin{bmatrix}\\mathbf{u} \\\\ \\boldsymbol{\\varepsilon} \\end{bmatrix} \\sim \\left( \\begin{bmatrix}\\boldsymbol{0} \\\\ \\boldsymbol{0} \\end{bmatrix}, \\begin{bmatrix}\\mathbf{G} &amp; \\boldsymbol{0} \\\\ \\boldsymbol{0} &amp; \\mathbf{R} \\end{bmatrix} \\right),\\] where \\(\\mathbf{y}\\) is the observed response, \\(\\mathbf{X}\\) is the matrix with the explanatory variables, \\(\\mathbf{Z}\\) is the design matrix, \\(\\boldsymbol{\\beta}\\) is the vector containing the fixed-effects parameters, \\(\\mathbf{u}\\) is the vector containing the random effects parameters, \\(\\boldsymbol{\\varepsilon}\\) is the vector containing the residuals, \\(\\mathbf{G}\\) is the variance-covariance matrix of the random effects, and \\(\\mathbf{R}\\) is the variance-covariance matrix of the residuals. Typically, \\(\\mathbf{G} = \\sigma^2_u \\mathbf{I}\\) and \\(\\mathbf{R} = \\sigma^2 \\mathbf{I}\\). If we do the math, we get that \\[E(\\mathbf{y}) = \\mathbf{X}\\boldsymbol{\\beta},\\] \\[Var(\\mathbf{y}) = \\mathbf{Z}\\mathbf{G}\\mathbf{Z}&#39; + \\mathbf{R}.\\] Fixed effects versus random effects Fixed vs Random Effects Table table.unique-table { width: 100%; border-collapse: collapse; margin: 20px 0; } table.unique-table th, table.unique-table td { border: 1px solid #ddd; padding: 8px; text-align: left; } table.unique-table th { background-color: #f4f4f4; font-weight: bold; } table.unique-table tr:nth-child(even) { background-color: #f9f9f9; } Fixed effects Random effects Where Expected value Variance-covariance matrix Inference Constant for all groups in the population of study Differ from group to group Usually used to model Carefully selected treatments or genotypes The study design (aka structure in the data, or what is similar to what) Assumptions \\[\\hat{\\boldsymbol{\\beta}} \\sim N \\left( \\boldsymbol{\\beta}, (\\mathbf{X}^T \\mathbf{V}^{-1} \\mathbf{X})^{-1} \\right) \\] \\[u_j \\sim N(0, \\sigma^2_u)\\] Method of estimation Maximum likelihood, least squares Restricted maximum likelihood (shrinkage) 4.2 Model checking and comparison Important things to keep in mind: Statistical models to analyze data generated by designed experiments. Models created to explain vs. models created to predict. See Shmueli (2010). 4.3 Model checking 4.4 Simulation-based model-checking Goal: to detect systematic differences between the model and observed data. See Chapter 8 in Gelman and Hill. library(lme4) library(tidyverse) library(latex2exp) library(ggpubr) dat &lt;- read.csv(&quot;../data/N_fert.csv&quot;) m1 &lt;- lm(Yield_SY ~ Total_N, data= dat) dat$residuals &lt;- resid(m1) dat$yhat &lt;- predict(m1) theme_set(theme_pubr()) y_sim &lt;- simulate(m1, nsim =30, seed = 42) |&gt; mutate(Total_N = dat$Total_N, Yield_SY = dat$Yield_SY) |&gt; pivot_longer(cols = -c(Total_N, Yield_SY)) y_sim |&gt; ggplot(aes(value))+ geom_histogram()+ geom_histogram(aes(Yield_SY), fill = &quot;gold&quot;, alpha = .3)+ facet_wrap(~name) ## `stat_bin()` using `bins = 30`. Pick better value `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value `binwidth`. y_sim |&gt; ggplot(aes(Total_N, value))+ geom_point()+ geom_point(aes(Total_N, Yield_SY), shape = 21, fill = &quot;gold&quot;, alpha = .3)+ facet_wrap(~name) Discuss the plot above: how are the simulated data compared to the observed data? 4.5 Some useful metrics to compare models 4.5.1 Root mean squared error Root mean squared error \\(RMSE = \\sqrt{\\frac{1}{n} \\cdot \\sum_{i=1}^n(y_i-\\hat{y}_i)^2}\\) In-sample versus out-of-sample RMSE 4.5.2 The coefficient of determination R2 Usually interpreted as the proportion of the variation in \\(y\\) that is explained with the variation in \\(x\\). Used as a metric for predictive ability and model fit. Can increase when adding more predictors. The R2 of a given model (and observed data) is calculated as \\[R^2 = \\frac{MSS}{TSS}= 1 - \\frac{RSS}{TSS} = 1- \\frac{MSE}{MST},\\] where \\(RSS\\) is the residual sum of squares and \\(TSS\\) is the total sum o squares, and \\(MSE\\) is the mean squared error and \\(MST\\) is the mean squared of the data (i.e., \\(y\\) versus \\(\\bar{y}\\). 4.5.3 Adjusted R2 The adjusted R2 also penalizes the addition of extra parameters \\[R^2_{adj} = R^2 - (1 - R^2) \\frac{p-1}{n-p},\\] where \\(R^2\\) is the one defined above, \\(p\\) is the number of parameters and \\(n\\) is the total number of observations. 4.5.4 Some issues with R2 Bootstrapped R2 Anscombe’s quartet Out-of-Sample R2: Estimation and Inference 4.5.5 Akaike Information Criterion (AIC) Used as a metric for predictive ability and model fit. Lower value is better. Values are always compared to other models (i.e., there are no general rules about reasonable AIC values). The AIC of a given model \\(M\\) and observed data \\(\\mathbf{y}\\) is calculated as \\[AIC_M = 2p - 2\\log(\\hat{L}),\\] \\(p\\) is the number of parameters estimated in the model and \\(\\hat{L}\\) is the maximized value of the likelihood function for the model (i.e., \\(\\hat{L}=p(\\mathbf{y}|\\hat{\\boldsymbol\\beta}, M)\\)). 4.5.6 Bayesian Information Criterion (BIC) The BIC of a given model (and observed data) is a variant of AIC and is calculated as \\[BIC = p\\log(n) - 2\\log(\\hat{L}),\\] where \\(p\\) is the number of parameters estimated in the model, \\(n\\) is the number of observations, and \\(\\hat{L}\\) is the maximized value of the likelihood function for the model (i.e., \\(\\hat{L}=p(\\mathbf{y}|\\hat{\\boldsymbol\\beta}, M)\\)). 4.6 Analysis of variance – ANOVA One of the oldest methods to analyze and interpret the data. knitr::include_graphics(&quot;figures/Ronald_Fisher.jpg&quot;) Figure 4.1: Sir R.A. Fisher, the father of the ANOVA. Helpful for understanding a previously fit model: Divide the sources of variability in “batches”. Quantify (&amp; test) said sources of variability to see which ones are more relevant to characterize the data. Some frequent assumptions: Linearity Normality Constant variance Discuss independence 4.6.1 This is how you build an ANOVA table The “What would Fisher do” (WWFD) approach Name the sources of variability and classify them into treatment sources of variability and ‘topographical’ (or logistical) sources. Assign degrees of freedom to each category. Combine both Sources (treatment + topographical) into a single ANOVA table. WWFD in the whiteboard Table 4.1: Constructing the ANOVA skeleton Table 4.1: Experiment or Topographical Source df Block b-1 - Fungicide(Block) (f-1)*b - - Gen(Fung x Block) (g-1)fb Total N-1 Table 4.1: Treatment Source df - - Fungicide f-1 - Genotype g-1 Fung x Gen (f-1)(g-1) Parallels N-(f*g) Total N-1 Table 4.1: Combined Table Source df Block b-1 Fungicide t-1 Fungicide(Block) (f-1)*b - (t-1) Genotype g-1 Fung x Gen (f-1)(g-1) Pens(Block x Trt) error (g-1)* f * b - (g-1 + (f-1)(g-1)) Total N-1 4.6.2 The elements of the ANOVA ANOVA table for the cookie split-plot experiment. table { width: 100%; border-collapse: collapse; margin: 20px 0; } th, td { border: 1px solid #ddd; padding: 8px; text-align: left; } th { background-color: #f4f4f4; font-weight: bold; } tr:nth-child(even) { background-color: #f9f9f9; } Source df SS MS EMS Block \\(b-1\\) \\[\\sigma^2_{\\varepsilon}+g\\sigma^2_w+tg\\sigma^2_d\\] Fungicide \\(t-1\\) \\(SS_{F}\\) \\(\\frac{SS_{F}}{b-1}\\) \\[\\sigma^2_{\\varepsilon}+g\\sigma^2_w+\\phi^2(\\alpha)\\] Error(whole plot) \\((b-1)(t-1)\\) \\[\\sigma^2_{\\varepsilon}+g\\sigma^2_w\\] Genotype \\(g-1\\) \\(SS_{G}\\) \\(\\frac{SS_{G}}{g-1}\\) \\[\\sigma^2_{\\varepsilon}+\\phi^2(\\gamma)\\] \\(T \\times G\\) \\((t-1)(g-1)\\) \\(SS_{F \\times G}\\) \\(\\frac{SS_{F \\times G}}{(t-1)(g-1)}\\) \\[\\sigma^2_{\\varepsilon}+\\phi^2(\\alpha \\gamma)\\] Error(split plot) \\(t(b-1)(g-1)\\) \\(SSE\\) \\(\\frac{SSE}{t(b-1)(g-1)}\\) \\[\\sigma^2_{\\varepsilon}\\] knitr::kable(t_comb, caption = &quot;Combined Table&quot;) Table 4.2: Combined Table Source df Block b-1 Fungicide t-1 Fungicide(Block) (f-1)*b - (t-1) Genotype g-1 Fung x Gen (f-1)(g-1) Pens(Block x Trt) error (g-1)* f * b - (g-1 + (f-1)(g-1)) Total N-1 4.7 Critiques of ANOVA over the years Mostly for normal data (GLMM class tomorrow). Low interpretability beyond statistical significance or lack thereof [greater Sums of squares are not necessarily those with higher estimated underlying variance components]. Complicated specifications for unbalanced data. 4.8 A different take on ANOVA using multilevel modeling “ANOVA is still very relevant” – see Gelman (2005). “The essence of analysis of variance is in the structuring of the coefficients into batches—hence the notation \\(\\beta^{(m)}_j\\) – going beyond the usual linear model formulation that has a single indexing of coefficients \\(\\beta_j\\)” Still useful as exploratory data analysis A different approach Use a hierarchical formulation in which each batch of regression coefficients is modeled as a sample from a normal distribution with mean 0 and its own variance \\(\\sigma^2_m\\): \\[\\beta^{(m)}_j \\sim N(0, \\sigma^2_m)\\] for \\(j = 1,\\dots,J_m\\) for each batch \\(m = 1,\\dots,M\\), . starting point for assessing the relative importance of the effects \\(\\beta\\) in linear models Note: The standard deviation \\(\\sigma^2_m\\) (describing the superpopulation) characterizes the uncertainty for predicting a new coefficient from batch \\(m\\). For that reason, Gelman (2005) proposes using \\(s^2_m\\) (describing the finite population), 4.8.1 An applied example The data below were generated by an experiment studying the effect of different feed additives on a pig’s health in two different timepoints. The data were generated under a CRD. Classic ANOVA Under the classic CRD, we could analyze the data using a classic two-way ANOVA. Now, other than saying that all treatments have an impact on the response, the ANOVA doesn’t really tell us much. dat &lt;- read.csv(&quot;../data/blood_study_pigs.csv&quot;) |&gt; mutate(Day =as.factor(Day)) m_classic &lt;- lm(Serum_haptoglobin_mg.dL ~ Trt * Day , data = dat) car::Anova(m_classic, type = 2) ## Anova Table (Type II tests) ## ## Response: Serum_haptoglobin_mg.dL ## Sum Sq Df F value Pr(&gt;F) ## Trt 81720 5 80.049 &lt; 2.2e-16 *** ## Day 69495 1 340.369 &lt; 2.2e-16 *** ## Trt:Day 24352 5 23.854 &lt; 2.2e-16 *** ## Residuals 36751 180 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 A multilevel model-based ANOVA We elaborate this approach to ANOVA under a Bayesian approach, in order to quantify the uncertainty to estimate the variance components. # get Bayes-specific libraries library(brms) library(bayesplot) # fit the Bayesian model m_new &lt;- brm(Serum_haptoglobin_mg.dL ~ (1|Trt) + (1|Day) + (1|Trt:Day), prior = c(set_prior(&quot;student_t(3, 0, 50)&quot;, class = &quot;sd&quot;, group = &quot;Trt&quot;), set_prior(&quot;student_t(3, 0, 50)&quot;, class = &quot;sd&quot;, group = &quot;Day&quot;), set_prior(&quot;student_t(3, 0, 50)&quot;, class = &quot;sd&quot;, group = &quot;Trt:Day&quot;)), backend = &quot;cmdstanr&quot;, iter = 2000, silent = 2, data = dat) # check model diagnostics bayesplot::mcmc_trace(m_new) The code below gets the posteriors for the random effects and computes the \\(s^2_m\\). # get s2 for Day re_Day &lt;- as_draws_df(m_new) |&gt; dplyr::select(contains(&quot;r_Day[&quot;)) |&gt; as.matrix() s_Day &lt;- numeric(nrow(re_Day)) for (i in 1:nrow(re_Day)){ s_Day[i] &lt;- sd(re_Day[i,]) } # get s2 for Trt re_Trt &lt;- as_draws_df(m_new) |&gt; dplyr::select(contains(&quot;r_Trt[&quot;)) |&gt; as.matrix() s_Trt &lt;- numeric(nrow(re_Trt)) for (i in 1:nrow(re_Trt)){ s_Trt[i] &lt;- sd(re_Trt[i,]) } # get s2 for Trt x Day re_TrtxDay &lt;- as_draws_df(m_new) |&gt; dplyr::select(contains(&quot;r_Trt:Day&quot;)) |&gt; as.matrix() s_TrtxDay &lt;- numeric(nrow(re_TrtxDay)) for (i in 1:nrow(re_TrtxDay)){ s_TrtxDay[i] &lt;- sd(re_TrtxDay[i,]) } # Summarize all s2 results into quantiles finite_sds &lt;- data.frame(Trt = s_Trt, Day = s_Day, TrtxDay = s_TrtxDay) |&gt; rownames_to_column(&quot;iter&quot;) |&gt; pivot_longer(cols = Trt:TrtxDay, names_to = &quot;Source&quot;) |&gt; group_by(Source) |&gt; summarise(s_m_mean = mean(value), q025 = quantile(value, probs = c(0.025)), q25 = quantile(value, probs = c(0.25)), q75 = quantile(value, probs = c(0.75)), q975 = quantile(value, probs = c(0.975))) ggplot(finite_sds, aes(x = s_m_mean, y = factor(Source, levels = c(&quot;TrtxDay&quot;, &quot;Trt&quot;, &quot;Day&quot;)))) + geom_point(size = 4) + geom_segment(aes(x = q025, xend = q975, y = Source, yend = Source), linetype = &quot;solid&quot;) + geom_segment(aes(x = q25, xend = q75, y = Source, yend = Source), size = 1.5, linetype = &quot;solid&quot;) + labs( title = &quot;Hierarchical ANOVA Display -- finite population&quot;, x = &quot;Estimated SD of Effects&quot;, y = &quot;Source of Variation&quot; ) + theme_minimal() m_new_results &lt;- as.matrix(m_new) colnames(m_new_results) &lt;- str_replace(colnames(m_new_results), &quot;__Intercept&quot;, &quot;&quot;) colnames(m_new_results) &lt;- str_replace(colnames(m_new_results), &quot;sd_&quot;, &quot;&quot;) color_scheme_set(&quot;purple&quot;) mcmc_intervals(m_new_results, pars = c(&quot;Day&quot;, &quot;Trt&quot;, &quot;Trt:Day&quot;, &quot;sigma&quot;))+ labs( title = &quot;Hierarchical ANOVA Display -- superpopulation&quot;, x = &quot;Estimated SD of effects&quot;, y = &quot;Source of variability&quot;) Discuss interpretation of a classic ANOVA versus this approach (simultaneous vs. not) Opportunities &amp; limitations 4.9 Coming up tomorrow: GLMMs Homework "],["generalized-linear-mixed-models.html", "Day 5 Generalized linear mixed models 5.1 GLMMs 5.2 Applied example – picking distributions 5.3 Reading", " Day 5 Generalized linear mixed models February 3rd, 2026 5.1 GLMMs Learning objectives: What are GLMMs What is a distributional assumption Working with GLMMs 5.1.1 Review – building a statistical model 1 – Select a probability distribution for the data \\(\\mathbf{y}|\\mathbf{u}\\). 2 – Select a link function \\(g(\\cdot)\\), \\(\\boldsymbol\\eta = g(\\mathbf{y}|\\mathbf{u})\\). 3 – Define the (linear) predictor \\(\\boldsymbol\\eta = \\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{Z}\\mathbf{u}\\). knitr::include_graphics(&quot;figures/distributions.png&quot;) Figure 5.1: Probability distributions. knitr::include_graphics(&quot;figures/stroup_distributions.jpg&quot;) Figure 5.2: Properties of probability distributions. 5.1.2 Generalities of GLMMs The structure of GLMMs is very similar to the notation we were using for LMMs. We can generally describe them as: \\[y|\\mathbf{u} \\sim P(\\mu, \\; \\phi),\\] where: \\(y\\) is the data, \\(\\mathbf{u}\\) are the random effects, \\(\\mu\\) is the mean, \\(\\phi\\) is the dispersion. The linear predictor for \\(\\mu\\) can be described as \\(g(\\boldsymbol{\\mu}) = \\boldsymbol{\\eta} = \\mathbf{X} \\boldsymbol{\\beta} + Zu}\\), where \\(g(\\mu) = \\eta\\) is the link function applied to the expected value. 5.1.3 Implications for model fitting Least Squares Estimator is no longer Maximum Likelihood Estimator Variance is no longer \\(\\hat\\sigma^2 = \\frac{SSE}{df_e}\\) The whole concept of degrees of freedom is more diffuse ANOVA shells are still useful to analyze designs are number of independent, true, replicates The \\(Var(\\mathbf{y}|\\mathbf{u})\\) may be specified, but not specify the full likelihood (check out the properties above) Quasi-likelihood for modeling overdispersion or repeated measures in GLMMs: \\(E(\\mathbf{y} \\vert \\mathbf{b}) = \\boldsymbol{\\mu}\\vert \\mathbf{b}\\) \\(Var(\\mathbf{y} \\vert \\mathbf{b}) = \\mathbf{V}_{\\mu}^{1/2}\\mathbf{A}\\mathbf{V}_{\\mu}^{1/2}\\) 5.1.4 Model diagnostics Number of Orobanche seeds tested/germinated for two genotypes and two treatments. library(glmmTMB) dat &lt;- agridat::crowder.seeds m_binom &lt;- glmmTMB(cbind(germ, n-germ) ~ gen*extract + (1|plate), REML = TRUE, family = binomial, data = dat) # If we followed conventional model checking, we couldn&#39;t find out about anything dat$phat &lt;- predict(m_binom, type = &quot;response&quot;) dat$resid &lt;- dat$germ - (dat$phat * dat$n) plot(dat$phat, dat$resid) DHARMa::simulateResiduals(m_binom, plot = T) ## Object of Class DHARMa with simulated residuals based on 250 simulations with refit = FALSE . See ?DHARMa::simulateResiduals for help. ## ## Scaled residual values: 0.1374871 0.5518681 0.1637147 0.9230439 0.7149169 0.8244776 0.3351514 0.277819 0.879538 0.08237197 0.6173228 0.5894222 0.7512811 0.2211422 0.108248 0.6607457 0.05951533 0.5632577 0.4263507 0.897899 ... 5.2 Applied example – picking distributions A clinical field trial comparing the performance of different vaccines on the growth and survival of Atlantic salmon under standard production conditions. Treatment structure: two-way factorial (4 vaccines x 5 times) Design structure: completely randomized design. Experimental unit: fish Response: weight. url &lt;- &quot;https://raw.githubusercontent.com/stat870/fall2025/refs/heads/main/data/fish_vaccines.csv&quot; fish &lt;- read.csv(url) # plot the data fish %&gt;% ggplot(aes(day, wt))+ geom_point(aes(fill = factor(vaccine), group = factor(vaccine)), position = position_dodge(width = 60), shape=21, size =3.5, alpha =.7)+ labs(y = expression(Weight~(g~fish^{-1})), x = &quot;Time (days)&quot;)+ theme_classic()+ labs(fill=&quot;Vaccine treatment&quot;)+ theme(legend.position = &quot;bottom&quot;)+ scico::scale_fill_scico_d() Considering that the variance increases with the mean, a fair model for fish weight could be: \\[y_{ijk}|u_{ijk} \\sim P(\\mu_{ijk}, \\phi),\\\\ \\log(\\mu_{ijk}) = \\eta_{ijk} = \\eta_0 + V_i + T_j + VT_{ij} + u_{ijk},\\] where: \\(y_{ijk}\\) is the fish weight of the \\(k\\)th fish under the \\(i\\)th vaccine treatment at the \\(j\\)th timepoint, and (conditional on \\(u_{ijk}\\)) arises from a Gamma distribution with mean \\(\\mu_{ijk}\\) and dispersion \\(\\phi\\), \\(\\eta_{ijk}\\) is the linear predictor, \\(\\eta_0\\) is the overall mean mean of the linear predictor, \\(V_i\\) is the effect of the \\(i\\)th vaccine treatment, \\(T_j\\) is the effect of the \\(j\\)th timepoint, \\(VT_{ij}\\) is the interaction between the \\(i\\)th vaccine treatment and the \\(j\\)th timepoint, and \\(u_{ijk}\\) is the random effect for the fish weight of the \\(k\\)th fish under the \\(i\\)th vaccine treatment at the \\(j\\)th timepoint, that is accounting for the fact that repeated measures are not independent. fish &lt;- fish |&gt; mutate(across(c(vaccine, day), ~as.factor(.))) |&gt; drop_na() m1_Normal &lt;- glmmTMB(wt ~ day*vaccine, data = fish) m1_Gamma &lt;- glmmTMB(wt ~ day*vaccine, family = Gamma(link = &quot;log&quot;), data = fish) m1_log &lt;- lm(log(wt) ~ day*vaccine, data = fish) fish$residual_N &lt;- resid(m1_Normal, type = &quot;response&quot;) fish$yhat_N &lt;- predict(m1_Normal, type = &quot;response&quot;) fish$residual_G &lt;- resid(m1_Gamma, type = &quot;response&quot;) fish$yhat_G &lt;- predict(m1_Gamma, type = &quot;response&quot;) simulate_N &lt;- simulate(m1_Normal) simulate_N$day &lt;- fish$day simulate_G &lt;- simulate(m1_Gamma) simulate_G$day &lt;- fish$day simulate_log &lt;- simulate(m1_log) simulate_log$day &lt;- fish$day fish |&gt; ggplot(aes(day, wt))+ geom_point(aes(day, sim_1), shape = 21, data = simulate_N, size = 7)+ geom_point(shape =21, fill =&quot;gold&quot;, size = 3)+ theme_pubr()+ labs(title = &quot;Observed weights in time (gold) and simulated weights based on the fitted Normal model (open)&quot;) fish |&gt; ggplot(aes(day, wt))+ geom_point(aes(day, sim_1), shape = 21, data = simulate_G, size = 7)+ geom_point(shape =21, fill =&quot;gold&quot;, size = 3)+ theme_pubr()+ labs(title = &quot;Observed weights in time (gold) and simulated weights based on the fitted Gamma model (open)&quot;) fish |&gt; ggplot(aes(day, wt))+ geom_point(aes(day, exp(sim_1)), shape = 21, data = simulate_log, size = 7)+ geom_point(shape =21, fill =&quot;gold&quot;, size = 3)+ theme_pubr()+ labs(title = &quot;Observed weights in time (gold) and simulated weights based on the fitted Gamma model (open)&quot;) library(ggpubr) 5.3 Reading Stats majors: Chapter 5 in Stroup et al. (2024) Non-stats majors: LMMs and GLMMs chapter by Ben Bolker The value of generalized linear mixed models for data analysis in the plant sciences Generalized Linear Mixed Models in Dairy Cattle Breeding "],["software-implementation-troubleshooting.html", "Day 6 Software implementation &amp; troubleshooting 6.1 Software implementation 6.2 Troubleshooting 6.3 In-class example 6.4 Coming up tomorrow:", " Day 6 Software implementation &amp; troubleshooting February 4th, 2026 6.1 Software implementation Writing statistical models Fitting linear mixed models using lme4 6.2 Troubleshooting lme4 convergence warnings: troubleshooting lme4’s own troubleshooting GLMM FAQ by Ben Bolker WTF – What They Forgot to Teach You About R 6.3 In-class example get R code 6.4 Coming up tomorrow: Non-linear mixed models "],["non-linear-mixed-models.html", "Day 7 Non-linear mixed models 7.1 The general linear model 7.2 Revisiting linearity 7.3 Some benefits of non-linear models 7.4 Applied example 7.5 Numeric and categorical predictors", " Day 7 Non-linear mixed models February 5th, 2026 7.1 The general linear model \\[\\mathbf{y} \\sim N(\\boldsymbol\\mu, \\boldsymbol\\Sigma),\\] where: \\(\\mathbf{y} \\equiv [y_1, y_2, \\dots, y_n]&#39;\\) contains the response data, \\(\\boldsymbol{\\mu} \\equiv [\\mu_1, \\mu_2, \\dots, \\mu_n]&#39;\\) contains the expected values of said data, \\(\\boldsymbol\\Sigma\\) is the variance-covariance matrix. The most typical model typically has: \\(\\boldsymbol\\mu = \\mathbf{X}\\boldsymbol{\\beta}\\) and \\(\\boldsymbol\\Sigma = \\sigma^2\\mathbf{I}\\). We can write the default model in most software written above as: \\[\\mathbf{y} \\sim N(\\boldsymbol{\\mu}, \\Sigma),\\\\ \\begin{bmatrix}y_1 \\\\ y_2 \\\\ y_3 \\\\ y_4 \\\\ \\vdots \\\\ y_n \\end{bmatrix} \\sim N \\left( \\begin{bmatrix}\\mu_1 \\\\ \\mu_2 \\\\ \\mu_3 \\\\ \\mu_4 \\\\ \\vdots \\\\ \\mu_n \\end{bmatrix}, \\sigma^2 \\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 &amp; \\dots &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 &amp; \\dots &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; \\dots &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; \\dots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots\\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\dots &amp; 1 \\end{bmatrix} \\right),\\] which is the same as \\[\\begin{bmatrix}y_1 \\\\ y_2 \\\\ y_3 \\\\ y_4 \\\\ \\vdots \\\\ y_n \\end{bmatrix} \\sim N \\left( \\begin{bmatrix}\\mu_1 \\\\ \\mu_2 \\\\ \\mu_3 \\\\ \\mu_4 \\\\ \\vdots \\\\ \\mu_n \\end{bmatrix}, \\begin{bmatrix} \\sigma^2 &amp; 0 &amp; 0 &amp; 0 &amp; \\dots &amp; 0 \\\\ 0 &amp; \\sigma^2 &amp; 0 &amp; 0 &amp; \\dots &amp; 0 \\\\ 0 &amp; 0 &amp; \\sigma^2 &amp; 0 &amp; \\dots &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; \\sigma^2 &amp; \\dots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots\\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\dots &amp; \\sigma^2 \\end{bmatrix} \\right).\\] In summary, the assumptions are: Linearity Normality Independence Constant variance 7.2 Revisiting linearity Assume that \\(y_i \\sim N(\\mu_i, \\sigma^2)\\). …and there are 4 possible descriptions of the mean: \\(\\mu_i = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i}\\) \\(\\mu_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2\\) \\(\\mu_i = \\beta_0 \\cdot \\exp(x_i \\cdot \\beta_1)\\) \\(\\mu_i = \\beta_0 + x_i^{\\beta_1}\\) What is a linear model and what is not? 7.3 Some benefits of non-linear models Process-based models versus non-process based models Process-based models are required to manage ecological systems in a changing world Some agronomy examples: Modeling biomass accumulation in maize kernels Nonlinear regression models and applications 7.4 Applied example library(tidyverse) library(lme4) url &lt;- &quot;https://raw.githubusercontent.com/stat799/spring2026/refs/heads/main/data/N_fert.csv&quot; data_N &lt;- read.csv(url) |&gt; mutate(sy = factor(sy)) data_N |&gt; ggplot(aes(Total_N, Yield_SY))+ geom_point(aes(fill = sy),size=2.5, shape = 21)+ scico::scale_fill_scico_d()+ labs(x = expression(N[fert]~(kg~ha^{-1})), y = expression(Yield~(kg~ha^{-1})), fill = &quot;Site-Year&quot;) m1 &lt;- nls(Yield_SY ~ Ymax - s * pmax(Ncrit - Total_N, 0), start = list(Ncrit = 150, Ymax = 14000, s =100), data = data_N) m2 &lt;- nls(Yield_SY ~ Ymax[sy] - s[sy] * pmax(Ncrit[sy] - Total_N, 0), start = list(Ncrit = rep(150, n_distinct(data_N$sy)), Ymax = rep(14000, n_distinct(data_N$sy)), s = rep(100, n_distinct(data_N$sy))), data = data_N) library(nlme) m3 &lt;- nlme(Yield_SY ~ Ymax - s * pmax(Ncrit - Total_N, 0), data = data_N, fixed = Ncrit + Ymax + s ~ 1, random = Ncrit + Ymax + s ~ 1 | sy, start = c(Ncrit = 150, Ymax = 14000, s =100)) df_pred &lt;- expand.grid(Total_N = seq(min(data_N$Total_N), max(data_N$Total_N),by=5)) library(marginaleffects) y_pred &lt;- predictions(m3, newdata = df_pred, level = 0) ## Warning: These arguments are not known to be supported for models of class `nlme`: level. All arguments ## are still passed to the model-specific prediction function, but users are encouraged to check if the ## argument is indeed supported by their modeling package. Please file a request on Github if you believe ## that an unknown argument should be added to the `marginaleffects` white list of known arguments, in order ## to avoid raising this warning: https://github.com/vincentarelbundock/marginaleffects/issues as.data.frame(y_pred) |&gt; ggplot(aes(Total_N, estimate))+ geom_point(aes(Total_N, Yield_SY),alpha =.5, data = data_N, color = &quot;grey40&quot;)+ geom_ribbon(aes(ymin = conf.low, ymax = conf.high),alpha =.5, fill = &quot;lavender&quot;)+ geom_line()+ labs(x = expression(N[fert]~(kg~ha^{-1})), y = expression(Yield~(kg~ha^{-1}))) 7.5 Numeric and categorical predictors What is the difference in terms of inference? Is there one general “best practice”? "],["statistical-inference.html", "Day 8 Statistical inference 8.1 What is statistical inference? 8.2 The components that go into interpreting results (in the context of LMMs) 8.3 On the use of R2 for statistical inference 8.4 Next week", " Day 8 Statistical inference February 6th, 2026 8.1 What is statistical inference? “Use math to understand the world” From raw data to assumptions to understanding (?) Define target population Figure 8.1: Mind map in a research project. 8.2 The components that go into interpreting results (in the context of LMMs) 8.2.1 Estimation Uncertainty in estimation – where is it coming from? Contrasts Multiple comparisons Let’s revisit the Nitrogen example, where we describe yield \\(y_{ij}\\) (\\(i\\)th observation in \\(j\\)th location) as \\[y_{ij} \\sim N(\\mu_{ij}, \\sigma^2), \\\\ \\mu_{ij} = \\beta_{0j} + \\beta_1 x_{ij} + \\beta_2 x_{ij}^2. \\] library(tidyverse) library(lme4) library(ggpubr) library(mosaic) url &lt;- &quot;https://raw.githubusercontent.com/stat799/spring2026/refs/heads/main/data/N_fert.csv&quot; data_N &lt;- read.csv(url) |&gt; mutate(sy = as.factor(sy)) data_N |&gt; ggplot(aes(Total_N, Yield_SY))+ geom_point(size=2.5)+ labs(x = expression(N[fert]~(kg~ha^{-1})), y = expression(Yield~(kg~ha^{-1})))+ theme_pubr() m1 &lt;- lm(Yield_SY ~ Total_N + I(Total_N^2) + sy, data_N) DHARMa::simulateResiduals(m1, plot = T) ## Object of Class DHARMa with simulated residuals based on 250 simulations with refit = FALSE . See ?DHARMa::simulateResiduals for help. ## ## Scaled residual values: 0.168 0.54 0.004 0.904 0.764 0.428 0.02 0.18 0.596 0.868 0.744 0.744 0.352 0.748 0.116 0.676 0.08 0.256 0.46 0.768 ... There’s multiple aspects to consider when we want to interpret the data. What was the main objective of the study? Option A: Study the effect of N on yield Consider the mean \\(f(x) = \\mu_{ij} = \\beta_{0j} + \\beta_1 x_{ij} + \\beta_2 x_{ij}^2\\). The slope is then \\(f&#39;(x) = \\beta_1 + 2 \\beta_2 x_{ij}\\) The slope depends on the level of \\(N_{fert}\\) we are considering! Likewise, consider \\(se(\\hat{\\beta}_1 + 2 \\hat{\\beta}_2 x_{ij})\\). How can we estimate the variance of a combination of parameters? The delta method sets the asymptotic variance of a function \\(g(\\cdot)\\) of the parameters (\\(g(\\hat\\beta)\\)): \\(Var(g(\\hat\\beta)) \\approx \\nabla g(\\hat\\beta)^T Cov(\\hat\\beta) \\nabla g(\\hat\\beta),\\) where \\(\\nabla g(\\hat\\beta)\\) is the gradient (vector of first derivatives) of function \\(g(\\cdot)\\) with respect to the parameters. In this case \\(se(\\hat{\\beta}_1 + 2 \\hat{\\beta}_2 x_{ij}) = \\sqrt(\\widehat{Var}(\\hat\\beta_1)+ 4 x_{ij}^2\\widehat{Var}(\\hat\\beta_2) + 4x_{ij}\\widehat{Cov}(\\hat\\beta_2))\\) (covariance &lt;- vcov(m1)) ## (Intercept) Total_N I(Total_N^2) sy2 sy3 sy4 ## (Intercept) 53465.012101 -4.143965e+02 1.148989e+00 -2.762121e+04 -2.762121e+04 -2.787241e+04 ## Total_N -414.396458 8.777726e+00 -2.908876e-02 5.914528e-14 5.353022e-14 -7.348210e+00 ## I(Total_N^2) 1.148989 -2.908876e-02 1.044085e-04 -2.632058e-16 -2.310362e-16 4.571248e-02 ## sy2 -27621.209876 5.914528e-14 -2.632058e-16 5.524242e+04 2.762121e+04 2.762121e+04 ## sy3 -27621.209876 5.353022e-14 -2.310362e-16 2.762121e+04 5.524242e+04 2.762121e+04 ## sy4 -27872.409930 -7.348210e+00 4.571248e-02 2.762121e+04 2.762121e+04 5.714695e+04 ## sy5 -27621.209876 3.743372e-14 -1.754706e-16 2.762121e+04 2.762121e+04 2.762121e+04 ## sy5 ## (Intercept) -2.762121e+04 ## Total_N 3.743372e-14 ## I(Total_N^2) -1.754706e-16 ## sy2 2.762121e+04 ## sy3 2.762121e+04 ## sy4 2.762121e+04 ## sy5 5.524242e+04 msm::deltamethod(g = ~ x2 + (2*x3*0), mean = coef(m1), cov = covariance) ## [1] 2.962723 msm::deltamethod(g = ~ x2 + (2*x3*100), mean = coef(m1), cov = covariance) ## [1] 1.148286 msm::deltamethod(g = ~ x2 + (2*x3*200), mean = coef(m1), cov = covariance) ## [1] 1.487304 We can also do this with the emmeans package: emtrends(m1, ~ Total_N, &quot;Total_N&quot;, at = list(Total_N = c(0, 100, 200))) ## Total_N Total_N.trend SE df lower.CL upper.CL ## 0 61.99 2.96 151 56.14 67.8 ## 100 35.88 1.15 151 33.62 38.1 ## 200 9.77 1.49 151 6.83 12.7 ## ## Results are averaged over the levels of: sy ## Confidence level used: 0.95 Option B: Study the effect of N on yield If we set \\(f&#39;(x) =0\\) and solve for \\(x\\), we can get the optimum rate. \\(ONR = -\\frac{\\beta_1}{2\\beta_2}\\) Consider the uncertainty behind \\(-\\frac{\\beta_1}{2\\beta_2}\\). (onr_hat &lt;- -coef(m1)[2]/(2*coef(m1)[3])) ## Total_N ## 237.5568 Same thing, we can get SE using the delta method: (onr_se_hat &lt;- msm::deltamethod(g = ~ -x2/(2*x3), mean = coef(m1), cov = covariance)) ## [1] 8.307668 8.2.2 Prediction Uncertainty in predictions Conditional distribution Marginal distribution Recall \\[y_{ij} \\sim N(\\mu_{ij}, \\sigma^2), \\\\ \\mu_{ij} = \\beta_{0j} + \\beta_1 x_{ij} + \\beta_2 x_{ij}^2. \\] Where is the uncertainty coming from? How can the uncertainty change depending on how \\(\\beta_{0j}\\) is specified? Model evaluation criteria for prediction-oriented models 8.3 On the use of R2 for statistical inference Why do so many people use the coefficient of variation R2? Metrics like RMSE are hard to compare across models (different units) R2, however, is dimensionless and much easier to interpret “The proportion of variability that is explained by the model” R2 can be computed as \\[R^2 = \\frac{SSM}{SST} = \\frac{SS_{MODEL}}{SS_{INT-SLOPE}} = 1 - \\frac{SSE}{SST},\\] where \\(R^2\\) is the coefficient of variation, \\(SSM\\) is the sum of squares of the model, \\(SST\\) is the total sum of squares (i.e., the residual sum of squares of an intercept-and-slope model). In essence, \\(R^2\\) is the ratio between the \\(SSE\\) of whatever model and the \\(SSE\\) of an intercept-and-slope model. Uncertainty in R2 summary(m1)$r.squared ## [1] 0.8966592 8.3.1 Bootstrapped R2 boot_r2 &lt;- do(1000) * { m_boot &lt;- lm(Yield_SY ~ Total_N + I(Total_N^2), data = resample(data_N, size = nrow(data_N), replace = TRUE)) summary(m_boot)$r.squared } hist(boot_r2$result) 8.3.2 Out-of-sample R2 R2_oos &lt;- numeric() for (i in unique(data_N$sy)) { # leave-one-sy-out d_is &lt;- data_N |&gt; filter(sy != i) d_oos &lt;- data_N |&gt; filter(sy == i) m &lt;- lm(Yield_SY ~ Total_N + I(Total_N^2), data = d_is) SST_oos &lt;- sum((resid(lm(Yield_SY ~ 1, data =d_oos)))^2) SSE_oos &lt;- sum((d_oos$Yield_SY - predict(m, newdata = d_oos))^2) R2_oos[i] &lt;- 1 - SSE_oos/SST_oos } R2_oos ## 1 4 2 3 5 ## 0.7341245 0.8376546 0.3494678 0.3219911 0.9103108 mean(R2_oos) ## [1] 0.6307098 sd(R2_oos) ## [1] 0.2766323 Compare the R2 8.4 Next week Monday 2.30 pm HW1 due WEDNESDAY Thursday 2.30pm "],["communicating-results.html", "Day 9 Communicating results 9.1 Scientific writing 9.2 Communicating statistical analyses 9.3 Communicating results and uncertainty 9.4 Reading", " Day 9 Communicating results February 9th, 2026 9.1 Scientific writing Introduction More general –&gt; more specific Last paragraph states the knowledge gap, and clearly mentions objectives. Using wording like “the objectives were …” Structure of a scientific paper (Figure 1 in Turbek et al., 2016) M&amp;M Reproducibility Reproducibility of the experiment Reproducibility of the statistical model Reproducible environments Results Highlight important results Discussion Put results in context Conclusion Connect the topic sentences in the discussion Writing Center (KSU) Scientific papers 9.1.1 Paragraphs One idea per paragraph One (bigger) idea per paper Structure of a paragraph [link] Topic sentence - presents the idea/claim of the paragraph Evidence Analysis Conclusion Structure of a paragraph 9.2 Communicating statistical analyses Discipline-specific criteria Statistical notation versus rhetoric Some examples from this class 9.3 Communicating results and uncertainty Communicating point estimates is relatively easy. So far, we can consider the point estimate the value with maximum likelihood to have generated the data. If we had to bet on a particular number, that would probably be the point estimate. Now, if we had to bet on a range of values, the width of that range would depend on the information in the data. Confidence intervals Confidence intervals are one of the most frequently mis-interpreted quantities. A safe approach: “The mean is between CIlow and CIhigh [units], with 95% confidence.” Statistical tests, P values, confidence intervals, and power: a guide to misinterpretation [link] library(tidyverse) library(lme4) library(emmeans) 9.3.1 Example The data below come from a split-plot designed experiment in complete blocks, with fungicide in the whole plot and genotype in the split plot. The model is \\[y_{ijk} | b_k, u_{j(k)} \\sim N(\\mu_{ijk}, \\sigma^2_\\varepsilon), \\\\ \\mu_{ijk} = \\mu + G_i + F_j + GF_{ij} + b_k + u_{j(k)}, \\\\ b_k \\sim N(0, \\sigma^2_b) , \\ u_{j(k)} \\sim N(0, \\sigma^2_u),\\] where \\(y_{ijk}\\) is the data that we assume is normally distributed, \\(\\mu_{ijk}\\) is the mean and \\(\\sigma^2_\\varepsilon\\) is the variance. The linear predictor of \\(\\mu_{ijk}\\) is the sum of an overall mean \\(\\mu\\), the effect of the \\(i\\)th genotype \\(G_i\\), the effect of the \\(j\\)th fungicide \\(F_j\\), the interaction between the \\(i\\)th genotype and \\(j\\)th fungicide \\(GF_{ij}\\), the random effect of the \\(k\\)th block \\(b_k\\), and the random whole-plot effect of the \\(j\\)th fungicide in the \\(k\\)th block \\(u_{j(k)}\\). Both random effects are assumed independent to each other and to the residual. dat_multilevel &lt;- agridat::durban.splitplot m_multilevel &lt;- lmer(yield ~ gen*fung + (1|block/fung), data = dat_multilevel) fung_mgmeans &lt;- emmeans(m_multilevel, ~fung) ## NOTE: Results may be misleading due to involvement in interactions as.data.frame(fung_mgmeans) |&gt; ggplot(aes(emmean, fung))+ geom_errorbarh(aes(xmin = lower.CL, xmax = upper.CL), width = 0)+ geom_point(size = 4)+ theme_bw()+ labs(y = &quot;Fungicide&quot;, x = expression(Yield~(Mg~ha^{-1})))+ theme(aspect.ratio = .2) 9.3.2 Hypothesis tests and p-values Hypothesis test evaluate the probability of observing the current data under a null hypothesis. When that probability is very low, we consider that is it very unlikely that the data were generated by that hypothesis, and we move on to an alternative hypothesis. What we need: A null hypothesis \\(H_0\\) representing the current paradigm or beliefs An alternative hypothesis \\(H_a\\) What we get: An answer rejecting (or not) the null hypothesis. Things to consider: The question reflected in some \\(H_0\\) is not always the real research question. The Difference Between “Significant” and “Not Significant” is not Itself Statistically Significant [link] ASA’s statement on p-values [link] Different types of hypothesis tests t-tests ANOVA – F test ~ multivariate t-test Post-hoc mean comparisons # simple marginal means + contrast (t test) emmeans(m_multilevel, ~fung, contr = list(c(1,-1))) ## $emmeans ## fung emmean SE df lower.CL upper.CL ## F1 5.51 0.105 4.18 5.23 5.80 ## F2 4.97 0.105 4.18 4.68 5.25 ## ## Results are averaged over the levels of: gen ## Degrees-of-freedom method: kenward-roger ## Confidence level used: 0.95 ## ## $contrasts ## contrast estimate SE df t.ratio p.value ## c(1, -1) 0.548 0.0863 3 6.346 0.0079 ## ## Results are averaged over the levels of: gen ## Degrees-of-freedom method: kenward-roger # ANOVA car::Anova(m_multilevel, type = 2, test.statistic = &quot;F&quot;) ## Analysis of Deviance Table (Type II Wald F tests with Kenward-Roger df) ## ## Response: yield ## F Df Df.res Pr(&gt;F) ## gen 7.2008 69 414 &lt; 2.2e-16 *** ## fung 40.2718 1 3 0.007915 ** ## gen:fung 0.9331 69 414 0.629010 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # multiple comparison library(multcomp) means_gen &lt;- emmeans(m_multilevel, ~gen) (mean_comparisons_gen &lt;- cld(means_gen, level = 0.05, adjust = &quot;sidak&quot;, Letters = letters)) ## gen emmean SE df lower.CL upper.CL .group ## G04 4.14 0.137 12.8 3.84 4.45 a ## G14 4.63 0.137 12.8 4.32 4.94 ab ## G20 4.80 0.137 12.8 4.49 5.11 bc ## G10 4.81 0.137 12.8 4.50 5.12 bcd ## G28 4.84 0.137 12.8 4.53 5.15 bcde ## G24 4.86 0.137 12.8 4.55 5.16 bcdef ## G01 4.91 0.137 12.8 4.60 5.22 bcdefg ## G05 4.94 0.137 12.8 4.63 5.25 bcdefgh ## G49 4.95 0.137 12.8 4.64 5.26 bcdefgh ## G34 5.01 0.137 12.8 4.70 5.32 bcdefghi ## G25 5.02 0.137 12.8 4.71 5.33 bcdefghi ## G30 5.04 0.137 12.8 4.73 5.35 bcdefghi ## G21 5.05 0.137 12.8 4.74 5.36 bcdefghi ## G02 5.06 0.137 12.8 4.75 5.37 bcdefghi ## G15 5.07 0.137 12.8 4.76 5.38 bcdefghi ## G41 5.09 0.137 12.8 4.78 5.40 bcdefghi ## G08 5.09 0.137 12.8 4.78 5.40 bcdefghi ## G07 5.10 0.137 12.8 4.79 5.41 bcdefghi ## G45 5.11 0.137 12.8 4.80 5.42 bcdefghi ## G06 5.13 0.137 12.8 4.82 5.44 bcdefghi ## G55 5.14 0.137 12.8 4.83 5.45 bcdefghij ## G32 5.16 0.137 12.8 4.85 5.47 bcdefghij ## G11 5.16 0.137 12.8 4.86 5.47 bcdefghij ## G39 5.18 0.137 12.8 4.87 5.49 bcdefghij ## G61 5.20 0.137 12.8 4.89 5.51 bcdefghij ## G53 5.21 0.137 12.8 4.90 5.52 bcdefghij ## G37 5.21 0.137 12.8 4.90 5.52 bcdefghij ## G26 5.21 0.137 12.8 4.90 5.52 bcdefghij ## G43 5.21 0.137 12.8 4.91 5.52 bcdefghij ## G35 5.22 0.137 12.8 4.91 5.53 bcdefghij ## G66 5.22 0.137 12.8 4.91 5.53 bcdefghij ## G16 5.23 0.137 12.8 4.92 5.54 bcdefghij ## G68 5.25 0.137 12.8 4.94 5.56 cdefghij ## G46 5.25 0.137 12.8 4.94 5.56 cdefghij ## G29 5.26 0.137 12.8 4.95 5.57 cdefghij ## G47 5.27 0.137 12.8 4.96 5.58 cdefghij ## G44 5.29 0.137 12.8 4.98 5.60 cdefghij ## G23 5.29 0.137 12.8 4.98 5.60 cdefghij ## G56 5.29 0.137 12.8 4.99 5.60 cdefghij ## G58 5.31 0.137 12.8 5.00 5.62 cdefghij ## G57 5.32 0.137 12.8 5.01 5.62 cdefghij ## G63 5.32 0.137 12.8 5.01 5.63 cdefghij ## G62 5.32 0.137 12.8 5.01 5.63 cdefghij ## G54 5.33 0.137 12.8 5.02 5.64 cdefghij ## G51 5.36 0.137 12.8 5.05 5.67 cdefghij ## G67 5.37 0.137 12.8 5.06 5.68 cdefghij ## G42 5.37 0.137 12.8 5.06 5.68 cdefghij ## G59 5.37 0.137 12.8 5.06 5.68 cdefghij ## G22 5.37 0.137 12.8 5.06 5.68 cdefghij ## G65 5.38 0.137 12.8 5.07 5.69 cdefghij ## G69 5.38 0.137 12.8 5.07 5.69 cdefghij ## G38 5.38 0.137 12.8 5.07 5.69 cdefghij ## G12 5.39 0.137 12.8 5.08 5.70 cdefghij ## G31 5.41 0.137 12.8 5.10 5.72 defghij ## G50 5.42 0.137 12.8 5.11 5.73 efghij ## G17 5.42 0.137 12.8 5.12 5.73 efghij ## G52 5.43 0.137 12.8 5.12 5.74 efghij ## G64 5.44 0.137 12.8 5.13 5.75 efghij ## G13 5.45 0.137 12.8 5.14 5.76 fghij ## G70 5.45 0.137 12.8 5.14 5.76 fghij ## G09 5.46 0.137 12.8 5.15 5.77 fghij ## G60 5.48 0.137 12.8 5.17 5.79 ghijk ## G27 5.49 0.137 12.8 5.18 5.80 ghijk ## G18 5.50 0.137 12.8 5.19 5.81 ghijk ## G36 5.50 0.137 12.8 5.20 5.81 ghijk ## G48 5.51 0.137 12.8 5.20 5.82 ghijk ## G40 5.53 0.137 12.8 5.22 5.84 hijk ## G33 5.61 0.137 12.8 5.30 5.92 ijk ## G19 5.73 0.137 12.8 5.42 6.04 jk ## G03 6.08 0.137 12.8 5.77 6.38 k ## ## Results are averaged over the levels of: fung ## Degrees-of-freedom method: kenward-roger ## Confidence level used: 0.05 ## Conf-level adjustment: sidak method for 70 estimates ## P value adjustment: sidak method for 2415 tests ## significance level used: alpha = 0.05 ## NOTE: If two or more means share the same grouping symbol, ## then we cannot show them to be different. ## But we also did not show them to be the same. 9.3.3 Other results in mixed-effects models Variance components Recall that \\[y_{ijk} | b_k, u_{j(k)} \\sim N(\\mu_{ijk}, \\sigma^2_\\varepsilon), \\\\ \\mu_{ijk} = \\mu + G_i + F_j + GF_{ij} + b_k + u_{j(k)}, \\\\ b_k \\sim N(0, \\sigma^2_b) , \\ u_{j(k)} \\sim N(0, \\sigma^2_u) \\] represents the conditional distribution of \\(y\\). At that level, the uncertainty is only coming from observing new data, \\(\\sigma^2_\\varepsilon\\). We could, however, obtain \\(y\\)s for all blocks and whole plots in general, where the uncertainty will represent (i) observing new data, (ii) observing new blocks, and (iii) observing new whole plots. VarCorr(m_multilevel) ## Groups Name Std.Dev. ## fung:block (Intercept) 0.11737 ## block (Intercept) 0.16972 ## Residual 0.28119 Discuss marginal inference versus conditional inference 9.4 Reading Writing Draft no. 4 (John McPhee) [link] Learning how to learn (Oakley et al.) [link] Data viz How to get better at embracing unknowns (J Hullman) [link] “The least effective way to present uncertainty is to not show it at all.” Visualizing Uncertainty About the Future (Spiegelhalter et al.) [link] Other Cross-validation strategies for data with temporal, spatial, hierarchical, or phylogenetic structure [link] "],["semester-project-1.html", "Day 10 Semester Project 10.1 Learning objectives 10.2 Partial deadlines", " Day 10 Semester Project Semester projects may deal with any topic that interests you [the student], as long as it is approved by the instructor. Broadly, projects are expected to identify a research problem and develop a designed experiment that is appropriate for solving that problem. Projects consist of a manuscript and a tutorial that describes the research problem, the experiment design and the treatment design. 10.1 Learning objectives Apply content learned in class in a real-life problem, connected to the student’s research program, including: Writing the materials and methods section of a paper using statistical notation. Writing detailed supporting material that clearly connects R code with statistical models. Interpret the results from the statistical model and connect them to the real-life problem. 10.2 Partial deadlines 10.2.1 Project proposal - Due Sunday February 1st at noon CT Write a 1 or 2 page-long project proposal that (a) includes the background of your problem (i.e., why is this important?) and (b) states your research objective. 10.2.2 Written report - Due Wednesday April 20 at 2pm CT for peer review Submit a manuscript to one of your classmates. The manuscript must include Abstract (250w), Introduction (~500w), M&amp;M (~300-500w), Results (~400w), Discussion (~400w), Conclusions (~100w). 10.2.3 Oral presentation - Somewhere between May 1 - May 9 Prepare a 15 minute presentation of the core aspects of your project. Presentations should include at least: Motivation/Background Methods, including a clear and complete description of the statistical model and code Discussion of strengths and weaknesses Conclusions 10.2.4 Written report and reproducible tutorial - Due May 15 Manuscript must include a publication-ready analysis. Must include Abstract (250w), Introduction (~500w), M&amp;M (~300-500w), Results (~400w), Discussion (~400w), Conclusions (~100w). Last paragraph in the Introduction should clearly state the research gap and the research objectives. Publication-ready tutorial/R documentation See evaluation rubric on Canvas. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
