[["index.html", "STAT 799 - Topics in Statistics: Applied linear mixed models in agriculture and life sciences Day 1 Welcome to STAT 799! 1.1 About this course: 1.2 Learning goals 1.3 On notation 1.4 Semester project 1.5 Roadmap of this course 1.6 Getting started with statistical modeling 1.7 Review 1.8 The most common statistical model 1.9 Uncertainty 1.10 Coming up tomorrow:", " STAT 799 - Topics in Statistics: Applied linear mixed models in agriculture and life sciences Josefina Lacasa Spring 2026 Day 1 Welcome to STAT 799! January 26th, 2026 1.1 About this course: About me About you: library(tidyverse) library(latex2exp) library(ggpubr) In rounds: What’s your major, what do you expect to learn? 1.1.1 Logistics Website Syllabus Statistical programming requirements Rough mindmap of the course (on whiteboard) Semester project Grades: Pass (100-69.99), Fail (&lt;69.99). Attendance policies Semester projects 1.2 Learning goals By the end of this course, you should be able to: Identify the data structure for a given dataset and write the statistical model that corresponds to said data structure using statistical notation. Distinguish the benefits and disadvantages of different modeling approaches. Write the Materials and Methods section in a paper (or graduate thesis) that describes the data generating process and the statistical model. 1.3 On notation scalars: \\(y\\), \\(\\sigma\\), \\(\\beta_0\\) vectors: \\(\\mathbf{y} \\equiv [y_1, y_2, ..., y_n]&#39;\\), \\(\\boldsymbol{\\beta} \\equiv [\\beta_1, \\beta_2, ..., \\beta_p]&#39;\\), \\(\\boldsymbol{u}\\) matrices: \\(\\mathbf{X}\\), \\(\\Sigma\\) probability distribution: \\(y \\sim N(0, \\sigma^2)\\), \\(\\mathbf{y} \\sim N(\\boldsymbol{0}, \\sigma^2\\mathbf{I})\\). 1.4 Semester project Manuscript See rubric on CANVAS. Publication-ready M&amp;M and results. Must include Abstract (250w), Introduction (~400-600w), M&amp;M (~300-700w), Results (~400w), Discussion (~300-500w), Conclusions (~100w). Last paragraph in the Introduction should clearly state the research gap and the research objectives. Reproducible Tutorial Publication-ready tutorial/R documentation. If you are not sure about your research question yet, talk to me after class today. 1.5 Roadmap of this course Table 1.1: Course Schedule: Statistical Modeling &amp; Mixed Models Date Topic Kahoot? 01/28 Statistical modeling. Continuous and categorical predictors. Review of mean, variance, and covariance. Types of uncertainty and sources of uncertainty. 01/29 Linear mixed models I. Model diagnostics and model selection. 01/30 Linear mixed models II. Model diagnostics and model selection. Yes Weekend NA NA 02/02 Analysis of variance. 02/03 Non-linear mixed models. 02/04 Troubleshooting in mixed models fitting using R software. Computational and analytical solutions. Yes 02/05 Generalized linear mixed models I. Beta, Binomial. 02/06 Statistical inference. Statistical power in designed experiments Weekend NA NA 02/09 Students are encouraged to attend Workshop ‘Non-linear models for the plant sciences’ 02/10 Students are encouraged to attend Workshop ‘Non-linear models for the plant sciences’ 02/11 Students are encouraged to attend Workshop ‘Non-linear models for the plant sciences’ 02/12 Scientific writing. Dos &amp; don’ts. 02/13 Wrap-up Yes 1.6 Getting started with statistical modeling Why do we need statistical models? Statistics as a summary of the data Excerpt from the short story “Funes the memorious” (J.L. Borges): I suspect, however, that he was not very capable of thought. To think is to forget differences, generalize, make abstractions. In the teeming world of Funes, there were only details, almost immediate in their presence. What is a statistical model? Deterministic component + Stochastic component 1.6.1 Writing a statistical model Deterministic component + Stochastic component Example: \\(y_i \\sim N(\\mu_i, \\sigma^2), \\\\ \\mu_i = \\beta_0 + \\beta_1 x_i\\) 1.6.2 What are linear models? Assume that \\(y_i \\sim N(\\mu_i, \\sigma^2)\\). …and there are 4 possible descriptions of the mean: \\(\\mu_i = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i}\\) \\(\\mu_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2\\) \\(\\mu_i = \\beta_0 \\cdot \\exp(x_i \\cdot \\beta_1)\\) \\(\\mu_i = \\beta_0 + x_i^{\\beta_1}\\) What is a linear model and what is not? 1.6.3 Vectorized notation Model equation form: \\(y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i\\) \\(\\varepsilon_i \\sim N(0, \\sigma^2),\\) Probability distribution form: Scalar \\(y_i \\sim N(\\mu_i, \\sigma^2),\\) \\(\\mu_i = \\beta_0 + \\beta_1 x_i\\) Vector \\(\\mathbf{y} \\sim N(\\boldsymbol{\\mu}, \\Sigma),\\) \\(\\boldsymbol{\\mu} = \\boldsymbol{1} \\beta_0 + \\mathbf{x} \\beta_1 = \\mathbf{X}\\boldsymbol{\\beta}\\) 1.6.4 Vectorized notation - cont. \\[\\begin{bmatrix}y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix} \\sim N \\left( \\begin{bmatrix}\\mu_1 \\\\ \\mu_2 \\\\ \\vdots \\\\ \\mu_n \\end{bmatrix}, \\begin{bmatrix} Cov(y_1, y_1) &amp; Cov(y_1, y_2) &amp; \\dots &amp; Cov(y_1, y_n) \\\\ Cov(y_2, y_1) &amp; Cov(y_2, y_2) &amp; \\dots &amp; Cov(y_2, y_n)\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ Cov(y_n, y_1) &amp; Cov(y_n, y_2) &amp; \\dots &amp; Cov(y_n, y_n) \\end{bmatrix} \\right)\\] 1.6.5 Continuous and categorical predictors \\[\\mu_i = \\beta_0 + x_{1i} \\beta_1 + x_{2i} \\beta_2+ ... + x_{ji} \\beta_j\\] Discuss the following type of data: biomass cultivar county year 1.6.6 Three-step approach to writing a statistical model 1. What probability distribution function describes the data? Think about: type of data, support of the data. 2. Define a link function for the linear predictor. 3. Define the linear predictor. Treatments Design 1.7 Review 1.7.1 Probability distributions knitr::include_graphics(&quot;figures/distributions.png&quot;) 1.7.2 Mean, Variance Mean: the balancing point of a probability distribution Variance: dispersion of \\(y\\) What do these distributions mean in practice? 1.7.3 Covariance Covariance between two random variables means how the two random variables behave relative to each other. Essentially, it quantifies the relationship between their joint variability. The variance of a random variable is the covariance of a random variable with itself. Consider two variables \\(y1\\) and \\(y2\\) each with a variance of 1 and a covariance of 0.6. \\[\\begin{bmatrix}y_1 \\\\ y_2 \\end{bmatrix} \\sim MVN \\left( \\begin{bmatrix} 10 \\\\ 8 \\end{bmatrix} , \\begin{bmatrix}1 &amp; 0.6 \\\\ 0.6 &amp; 1 \\end{bmatrix} \\right),\\] where the means of \\(y_1\\) and \\(y_2\\) are 10 and 8, respectively, and their covariance structure is represented in the variance-covariance matrix. Remember: \\[\\begin{bmatrix}y_1 \\\\ y_2 \\end{bmatrix} \\sim MVN \\left( \\begin{bmatrix} E(y_1) \\\\ E(y_2) \\end{bmatrix} , \\begin{bmatrix} Var(y_1) &amp; Cov(y_1, y_2) \\\\ Cov(y_2,y_2) &amp; Var(y_2) \\end{bmatrix} \\right).\\] 1.8 The most common statistical model \\[\\mathbf{y} \\sim N(\\boldsymbol\\mu, \\boldsymbol\\Sigma),\\] where: \\(\\mathbf{y} \\equiv [y_1, y_2, \\dots, y_n]&#39;\\) contains the response data, \\(\\boldsymbol{\\mu} \\equiv [\\mu_1, \\mu_2, \\dots, \\mu_n]&#39;\\) contains the expected values of said data, \\(\\boldsymbol\\Sigma\\) is the variance-covariance matrix. The most typical model typically has: \\(\\boldsymbol\\mu = \\mathbf{X}\\boldsymbol{\\beta}\\) and \\(\\boldsymbol\\Sigma = \\sigma^2\\mathbf{I}\\). In summary, the assumptions are: Normality Independence Constant variance 1.8.1 Properties of the general linear model \\(E(\\hat{\\boldsymbol{\\beta}}) = \\boldsymbol{\\beta}\\) \\(\\text{Var}(\\hat{\\boldsymbol{\\beta}}) = \\frac{\\sigma^2}{\\mathbf{X}^T\\mathbf{X}} = \\sigma^2 (\\mathbf{X}^T\\mathbf{X})^{-1}\\) Class discussion: What if the observations weren’t independent? What if the variance wasn’t constant? 1.9 Uncertainty Uncertainty will be one of the central topics in this course. Uncertainty is important because, as we summarize the information (e.g., to aviod Funes’ useless excess of information), we can better describe all the information with a combination of the mean and the variance. 1.9.1 Types of uncertainty Epistemological uncertainty Intrinsic uncertainty 1.9.2 Types of uncertainty in the general linear model Recall \\(\\mathbf{y} \\sim N(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\\), where \\(\\boldsymbol{\\mu} = \\mathbf{X}\\boldsymbol{\\beta}\\), and suppose \\(\\boldsymbol{\\Sigma} = \\sigma^2 \\mathbf{I}\\). We use the estimator of \\(\\boldsymbol{\\beta}\\), \\(\\hat{\\boldsymbol{\\beta}}\\). \\(Var(\\mathbf{y}) = \\sigma^2 = \\frac{SSE}{df_e}\\), \\(Var(\\hat{\\boldsymbol{\\beta}}) = \\frac{\\sigma^2}{\\mathbf{X}^T\\mathbf{X}}\\), and \\(Var(\\hat\\beta_1) = \\frac{\\sigma^2}{(n-1)s^2_{x_1}}\\). Discuss the connection between \\(n\\), \\({df_e}\\), \\(\\sigma^2\\) and \\(Var(\\hat{\\boldsymbol{\\beta}})\\). 1.9.3 Uncertainty - applied example Assume that the model \\(y_i \\sim N(\\mu_i, \\sigma^2), \\mu_i = \\beta_0 + \\beta_1 x_i+ \\beta_2 x_i^2\\) describes the data generating process. url &lt;- &quot;https://raw.githubusercontent.com/k-state-id3a/mixed-models-fall25/refs/heads/newpart3/data/nitrogen_yield.csv&quot; df_n_ss &lt;- read.csv(url) m1 &lt;- lm(Yield_SY ~ Total_N + I(Total_N^2), data = df_n_ss) DHARMa::simulateResiduals(m1, plot = T) ## Object of Class DHARMa with simulated residuals based on 250 simulations with refit = FALSE . See ?DHARMa::simulateResiduals for help. ## ## Scaled residual values: 0.1 0.3 0.004 0.588 0.792 0.556 0.168 0.344 0.256 0.452 0.388 0.388 0.832 0.94 0.692 0.936 0.132 0.312 0.424 0.664 ... m1 &lt;- lm(Yield_SY ~ Total_N + I(Total_N^2), data = df_n_ss) df_plot &lt;- data.frame(Total_N = 0:280) df_plot$yhat &lt;- predict(m1, newdata = df_plot) # estimation uncertainty confid_est &lt;- predict(m1, newdata = df_plot, se.fit = TRUE, interval = &quot;confidence&quot;) df_plot$est_low &lt;- (confid_est$fit)[,2] df_plot$est_up &lt;- (confid_est$fit)[,3] # estimation uncertainty confid_pred &lt;- predict(m1, newdata = df_plot, se.fit = TRUE, interval = &quot;predict&quot;) df_plot$pred_low &lt;- (confid_pred$fit)[,2] df_plot$pred_up &lt;- (confid_pred$fit)[,3] df_plot |&gt; ggplot(aes(Total_N, yhat))+ geom_ribbon(aes(ymin = pred_low, ymax = pred_up), alpha = .3, fill = &quot;#B388EB&quot;)+ geom_ribbon(aes(ymin = est_low, ymax = est_up), alpha = .6, fill = &quot;#B200EB&quot;)+ geom_line()+ labs(y = latex2exp::TeX(&quot;$\\\\hat{y}$&quot;), x = expression(Total~N~(lb~ac^{-1})))+ theme_pubclean() 1.10 Coming up tomorrow: Reach Ch1 in Stroup et al. (2024) We start with mixed models "],["mixed-models-i.html", "Day 2 Mixed models I 2.1 Recall the most common statistical model 2.2 Variations to that very common statistical model 2.3 Relaxing the assumption of independence 2.4 Applied examples 2.5 Model checking and comparison 2.6 Some useful metrics to compare models", " Day 2 Mixed models I January 27th, 2026 2.1 Recall the most common statistical model \\[\\mathbf{y} \\sim N(\\boldsymbol\\mu, \\boldsymbol\\Sigma),\\] where: \\(\\mathbf{y} \\equiv [y_1, y_2, \\dots, y_n]&#39;\\) contains the response data, \\(\\boldsymbol{\\mu} \\equiv [\\mu_1, \\mu_2, \\dots, \\mu_n]&#39;\\) contains the expected values of said data, \\(\\boldsymbol\\Sigma\\) is the variance-covariance matrix. The most typical model typically has: \\(\\boldsymbol\\mu = \\mathbf{X}\\boldsymbol{\\beta}\\) and \\(\\boldsymbol\\Sigma = \\sigma^2\\mathbf{I}\\). In summary, the assumptions are: Normality Independence Constant variance 2.2 Variations to that very common statistical model We can adapt the assumptions one by one: Normality – assume a different distribution Independence – implement a hierarchical/multi-level/mixed model that models the data that were generated together. Constant variance – model the variance/assume a different distribution where the mean and the variance are not independent. 2.3 Relaxing the assumption of independence What if we can model the variance-covariance matrix with something else that’s not \\(\\sigma^2 \\mathbf{I}\\)? library(tidyverse) my_theme &lt;- theme_minimal()+ theme(aspect.ratio = 1, axis.text = element_blank(), legend.position = &quot;bottom&quot;, panel.grid = element_blank(), axis.title = element_blank(), axis.ticks = element_blank()) theme_set(my_theme) sigma &lt;- 2 diag(sigma^2, nrow = 18) |&gt; as.data.frame() |&gt; rownames_to_column(&quot;x&quot;) |&gt; pivot_longer(cols = -c(x), names_to = &quot;y&quot;) |&gt; mutate(y = str_replace(y, &quot;V&quot;, &quot;&quot;) |&gt; as.numeric(), x = as.numeric(x)) |&gt; ggplot(aes(x,-y))+ geom_tile(aes(fill = value))+ scale_fill_viridis_c()+ labs(fill = &quot;variance/covariance&quot;) m_cs &lt;- lmer(diameter ~ time + (1|appleid), data = agridat::byers.apple |&gt; filter(appleid %in% c(1,4,32))) Z &lt;- getME(m_cs, &quot;Z&quot;) sigma2_s &lt;- .8 G &lt;- diag(c(rep(sigma2_s, 3))) Z %*% G %*% t(Z) %&gt;% as.matrix() %&gt;% as.data.frame() %&gt;% rownames_to_column(&quot;x&quot;) %&gt;% pivot_longer(cols = -c(x), names_to = &quot;y&quot;) %&gt;% mutate(x = as.numeric(x), y= as.numeric(y)) %&gt;% left_join( diag(sigma^2, nrow = 18) %&gt;% as.data.frame() %&gt;% rownames_to_column(&quot;x&quot;) %&gt;% pivot_longer(cols = -c(x), names_to = &quot;y&quot;) %&gt;% mutate(y = str_replace(y, &quot;V&quot;, &quot;&quot;) %&gt;% as.numeric(), x = as.numeric(x)) %&gt;% rename(s2 = value)) %&gt;% ggplot(aes(-x,y))+ geom_tile(aes(fill = value+s2))+ theme_minimal()+ scale_fill_viridis_c()+ labs(fill = &quot;variance/covariance&quot;)+ theme(aspect.ratio = 1, axis.text = element_blank(), legend.position = &quot;bottom&quot;, panel.grid = element_blank(), axis.title = element_blank(), axis.ticks = element_blank()) ## Joining with `by = join_by(x, y)` m_cs &lt;- lmer(diameter ~ time + (1|tree/appleid), data = agridat::byers.apple |&gt; filter(appleid %in% c(1,4,32))) ## boundary (singular) fit: see help(&#39;isSingular&#39;) Z &lt;- getME(m_cs, &quot;Z&quot;) sigma2_s &lt;- .8 sigma2_t &lt;- 1.2 sigma &lt;- 1.5 G &lt;- diag(c(rep(sigma2_s, 3), rep(sigma2_t, 2))) Z %*% G %*% t(Z) %&gt;% as.matrix() %&gt;% as.data.frame() %&gt;% rownames_to_column(&quot;x&quot;) %&gt;% pivot_longer(cols = -c(x), names_to = &quot;y&quot;) %&gt;% mutate(x = as.numeric(x), y= as.numeric(y)) %&gt;% left_join( diag(sigma^2, nrow = 18) %&gt;% as.data.frame() %&gt;% rownames_to_column(&quot;x&quot;) %&gt;% pivot_longer(cols = -c(x), names_to = &quot;y&quot;) %&gt;% mutate(y = str_replace(y, &quot;V&quot;, &quot;&quot;) %&gt;% as.numeric(), x = as.numeric(x)) %&gt;% rename(s2 = value)) %&gt;% ggplot(aes(-x,y))+ geom_tile(aes(fill = value+s2))+ theme_minimal()+ scale_fill_viridis_c()+ labs(fill = &quot;variance/covariance&quot;)+ theme(aspect.ratio = 1, axis.text = element_blank(), legend.position = &quot;bottom&quot;, panel.grid = element_blank(), axis.title = element_blank(), axis.ticks = element_blank()) ## Joining with `by = join_by(x, y)` 2.4 Applied examples Three sets of data describe the relationship between treatment and crop yield. However, the structures in the data (i.e., data architecture) are different. 2.4.1 Example A – independence holds dat_independent &lt;- read.csv(&quot;../data/cochrancox_kfert.csv&quot;) m_independent &lt;- lm(yield ~ factor(K2O_lbac), data = dat_independent) DHARMa::simulateResiduals(m_independent, plot=T) ## Object of Class DHARMa with simulated residuals based on 250 simulations with refit = FALSE . See ?DHARMa::simulateResiduals for help. ## ## Scaled residual values: 0.168 0.528 0.452 0.072 0.352 0.78 0.788 0.68 0.724 0.912 0.544 0.228 0.408 0.732 0.136 2.4.2 Example B – simple groups of similar observations library(lme4) dat_blocked &lt;- agridat::cochran.factorial m_blocked &lt;- lmer(yield ~ trt + (1|block), data = dat_blocked) DHARMa::simulateResiduals(m_blocked, plot=T) ## Object of Class DHARMa with simulated residuals based on 250 simulations with refit = FALSE . See ?DHARMa::simulateResiduals for help. ## ## Scaled residual values: 0.728 0.692 0.848 0.26 0.72 0.388 0.868 0.352 0.392 0.58 0.172 0.336 0.188 0.512 0.444 0.856 0.728 0.124 0.324 0.232 ... 2.4.3 Example C – different groups of similar observations dat_multilevel &lt;- agridat::durban.splitplot m_multilevel &lt;- lmer(yield ~ gen*fung + (1|block/fung), data = dat_multilevel) 2.5 Model checking and comparison Important things to keep in mind: Statistical models to analyze data generated by designed experiments. Models created to explain vs. models created to predict. 2.5.1 Model checking Distributional assumptions Variance assumptions Simulation-based model-checking Detect systematic differences between the model and observed data. 2.6 Some useful metrics to compare models Root mean squared error \\(RMSE = \\sqrt{\\frac{1}{n} \\cdot \\sum_{i=1}^n(\\hat{y}_i-y_i)^2}\\) In-sample versus out-of-sample RMSE R2 Why is R2 a suboptimal metric? In-sample versus out-of-sample R2 Akaike information criterion (AIC) Bayesian information criterion (BIC) "],["semester-project-1.html", "Day 3 Semester Project 3.1 Learning objectives 3.2 Partial deadlines", " Day 3 Semester Project Semester projects may deal with any topic that interests you [the student], as long as it is approved by the instructor. Broadly, projects are expected to identify a research problem and develop a designed experiment that is appropriate for solving that problem. Projects consist of a manuscript and a tutorial that describes the research problem, the experiment design and the treatment design. 3.1 Learning objectives Apply content learned in class in a real-life problem, connected to the student’s research program, including: Writing the materials and methods section of a paper using statistical notation. Writing detailed supporting material that clearly connects R code with statistical models. Interpret the results from the statistical model and connect them to the real-life problem. 3.2 Partial deadlines 3.2.1 Project proposal - Due Sunday February 1st at noon CT Write a 1 or 2 page-long project proposal that (a) includes the background of your problem (i.e., why is this important?) and (b) states your research objective. 3.2.2 Written report - Due Wednesday April 20 at 2pm CT for peer review Submit a manuscript to one of your classmates. The manuscript must include Abstract (250w), Introduction (~500w), M&amp;M (~300-500w), Results (~400w), Discussion (~400w), Conclusions (~100w). 3.2.3 Oral presentation - Somewhere between May 1 - May 9 Prepare a 15 minute presentation of the core aspects of your project. Presentations should include at least: Motivation/Background Methods, including a clear and complete description of the statistical model and code Discussion of strengths and weaknesses Conclusions 3.2.4 Written report and reproducible tutorial - Due May 15 Manuscript must include a publication-ready analysis. Must include Abstract (250w), Introduction (~500w), M&amp;M (~300-500w), Results (~400w), Discussion (~400w), Conclusions (~100w). Last paragraph in the Introduction should clearly state the research gap and the research objectives. Publication-ready tutorial/R documentation See evaluation rubric on Canvas. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
